{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4qMTHDxfeA1K"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DC-DBe_qeMhi"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kjuia1oReNza"
   },
   "outputs": [],
   "source": [
    "seed = 10\n",
    "\n",
    "def setup_seed(seed=seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "     torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('output.txt', 'a')\n",
    "f.write('seed: ' + str(seed) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eAB4y1QIePRh"
   },
   "outputs": [],
   "source": [
    "raw_ds_abnormal = pd.read_csv(r'~/Python_projects/Rationale/Dataset/abnormal_ds2.csv')\n",
    "raw_ds_normal = pd.read_csv(r'~/Python_projects/Rationale/Dataset/normal_ds2.csv')\n",
    "\n",
    "raw_ds_abnormal['Sequence'] = raw_ds_abnormal['Sequence'].apply(lambda x: x[1:-1].split())\n",
    "raw_ds_abnormal['Key_label'] = raw_ds_abnormal['Key_label'].apply(lambda x: x[1:-1].split())\n",
    "raw_ds_abnormal['Key_label'] = raw_ds_abnormal['Key_label'].apply(lambda x: np.array(list(map(int, x))))\n",
    "raw_ds_abnormal['URL'] = raw_ds_abnormal['URL'].apply(lambda x: x[1:-1].split())\n",
    "\n",
    "raw_ds_normal['Sequence'] = raw_ds_normal['Sequence'].apply(lambda x: x[1:-1].split())\n",
    "raw_ds_normal['Key_label'] = raw_ds_normal['Key_label'].apply(lambda x: x[1:-1].split())\n",
    "raw_ds_normal['Key_label'] = raw_ds_normal['Key_label'].apply(lambda x: np.array(list(map(int, x))))\n",
    "raw_ds_normal['URL'] = raw_ds_normal['URL'].apply(lambda x: x[1:-1].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GLgqsB_EeT0V"
   },
   "outputs": [],
   "source": [
    "test_normal_ds = raw_ds_abnormal[raw_ds_abnormal['Sequence_label']==0]\n",
    "test_abnormal_ds = raw_ds_abnormal[raw_ds_abnormal['Sequence_label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sdZen-UwfajL"
   },
   "outputs": [],
   "source": [
    "setup_seed()\n",
    "\n",
    "train_ds = raw_ds_normal\n",
    "test_normal_ds, val_normal_ds = train_test_split(test_normal_ds, test_size=0.1, random_state=2021)\n",
    "test_abnormal_ds, val_abnormal_ds = train_test_split(test_abnormal_ds, test_size=0.1, random_state=2021)\n",
    "\n",
    "test_ds = pd.concat([test_normal_ds, test_abnormal_ds])\n",
    "val_ds = pd.concat([val_normal_ds, val_abnormal_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.concat([train_ds, test_ds, val_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Sequence_label</th>\n",
       "      <th>Key_label</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Logon', 'Http_normal', 'Http_normal', 'Http_...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[nan, 'megaupload.com', 'megaclick.com', 'sfga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Http_normal', 'Http_normal', 'Http_normal', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>['megaupload.com', 'megaclick.com', 'sfgate.co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Http_normal', 'Http_normal', 'Http_normal', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>['megaclick.com', 'sfgate.com', 'sfgate.com', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Http_normal', 'Http_normal', 'Email', 'Email...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>['sfgate.com', 'sfgate.com', nan, nan, nan, 'n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['Http_normal', 'Email', 'Email', 'Email', 'Ht...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>['sfgate.com', nan, nan, nan, 'ning.com', nan,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688800</th>\n",
       "      <td>['Http_abnormal', 'Http_normal', 'Http_normal'...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>['simplyhired.com', 'etsy.com', 'homedepot.com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176469</th>\n",
       "      <td>['Http_normal', 'Http_normal', 'Http_normal', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>['pcworld.com', 'pcworld.com', 'pcworld.com', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690699</th>\n",
       "      <td>['Email', 'Http_normal', 'Http_normal', 'Email...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[nan, 'superpages.com', 'vistaprint.com', nan,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379270</th>\n",
       "      <td>['Http_normal', 'Http_normal', 'Http_normal', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>['thechive.com', 'tribalfusion.com', 'istockph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242724</th>\n",
       "      <td>['Disconnect', 'Email', 'Connect', 'Http_norma...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[nan, nan, nan, 'washingtonpost.com', nan, nan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2085552 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Sequence  Sequence_label  \\\n",
       "0       ['Logon', 'Http_normal', 'Http_normal', 'Http_...               0   \n",
       "1       ['Http_normal', 'Http_normal', 'Http_normal', ...               0   \n",
       "2       ['Http_normal', 'Http_normal', 'Http_normal', ...               0   \n",
       "3       ['Http_normal', 'Http_normal', 'Email', 'Email...               0   \n",
       "4       ['Http_normal', 'Email', 'Email', 'Email', 'Ht...               0   \n",
       "...                                                   ...             ...   \n",
       "688800  ['Http_abnormal', 'Http_normal', 'Http_normal'...               1   \n",
       "176469  ['Http_normal', 'Http_normal', 'Http_normal', ...               1   \n",
       "690699  ['Email', 'Http_normal', 'Http_normal', 'Email...               1   \n",
       "379270  ['Http_normal', 'Http_normal', 'Http_normal', ...               1   \n",
       "242724  ['Disconnect', 'Email', 'Connect', 'Http_norma...               1   \n",
       "\n",
       "                                                Key_label  \\\n",
       "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                   ...   \n",
       "688800  [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "176469  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "690699  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "379270  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "242724  [1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                                      URL  \n",
       "0       [nan, 'megaupload.com', 'megaclick.com', 'sfga...  \n",
       "1       ['megaupload.com', 'megaclick.com', 'sfgate.co...  \n",
       "2       ['megaclick.com', 'sfgate.com', 'sfgate.com', ...  \n",
       "3       ['sfgate.com', 'sfgate.com', nan, nan, nan, 'n...  \n",
       "4       ['sfgate.com', nan, nan, nan, 'ning.com', nan,...  \n",
       "...                                                   ...  \n",
       "688800  ['simplyhired.com', 'etsy.com', 'homedepot.com...  \n",
       "176469  ['pcworld.com', 'pcworld.com', 'pcworld.com', ...  \n",
       "690699  [nan, 'superpages.com', 'vistaprint.com', nan,...  \n",
       "379270  ['thechive.com', 'tribalfusion.com', 'istockph...  \n",
       "242724  [nan, nan, nan, 'washingtonpost.com', nan, nan...  \n",
       "\n",
       "[2085552 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-CC-KhPDJ8f"
   },
   "source": [
    "**2. Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KyvIiONflmX8"
   },
   "outputs": [],
   "source": [
    "dict_activity = {'': 0, \n",
    "                 \"UNK\":1, \n",
    "                 'Logon': 2, \n",
    "                 'Logoff': 3, \n",
    "                 'Connect': 4, \n",
    "                 'Disconnect': 5, \n",
    "                 'Email': 6, \n",
    "                 'Http_normal': 7, \n",
    "                 'Http_abnormal': 8, \n",
    "                 'File': 9}\n",
    "\n",
    "# dict_activity = {'': 0, \n",
    "#                  \"UNK\":1, \n",
    "#                  'Logon': 2, \n",
    "#                  'Logoff': 3, \n",
    "#                  'Connect_normal': 4, \n",
    "#                  'Connect_abnormal': 5, \n",
    "#                  'Disconnect_normal': 6, \n",
    "#                  'Disconnect_abnormal': 7,\n",
    "#                  'Email_normal': 8, \n",
    "#                  'Email_abnormal': 9,\n",
    "#                  'Http_normal': 10, \n",
    "#                  'Http_abnormal': 11, \n",
    "#                  'File': 12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "\n",
    "for index, row in ds.iterrows():\n",
    "    counts.update(row['URL'])\n",
    "\n",
    "url2index = {\"\":0,\"UNK\":1}\n",
    "urls = [\"\",\"UNK\"]\n",
    "\n",
    "for url in counts:\n",
    "    url2index[url] = len(urls)\n",
    "    urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whyFN9tPlvON",
    "outputId": "2447001c-5dc7-4f04-b404-fa1d404bd110",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encode_sequence(sequence, dict_activity):\n",
    "    return np.array([dict_activity.get(logkey[1:-1], dict_activity[\"UNK\"]) for logkey in sequence])\n",
    "\n",
    "def encode_sequence2(sequence, dict_activity):\n",
    "    return np.array([dict_activity.get(logkey, dict_activity[\"UNK\"]) for logkey in sequence])\n",
    "\n",
    "train_ds.loc[:,'Encoded'] = train_ds.loc[:,'Sequence'].apply(lambda x: encode_sequence(x,dict_activity))\n",
    "val_ds.loc[:,'Encoded'] = val_ds.loc[:,'Sequence'].apply(lambda x: encode_sequence(x,dict_activity))\n",
    "test_ds.loc[:,'Encoded'] = test_ds.loc[:,'Sequence'].apply(lambda x: encode_sequence(x,dict_activity))\n",
    "\n",
    "train_ds.loc[:,'URL'] = train_ds.loc[:,'URL'].apply(lambda x: encode_sequence2(x,url2index))\n",
    "val_ds.loc[:,'URL'] = val_ds.loc[:,'URL'].apply(lambda x: encode_sequence2(x,url2index))\n",
    "test_ds.loc[:,'URL'] = test_ds.loc[:,'URL'].apply(lambda x: encode_sequence2(x,url2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1ZfMwn-dm5U5"
   },
   "outputs": [],
   "source": [
    "train_data = train_ds[['Encoded', 'Sequence_label', 'Key_label', 'URL']]\n",
    "test_data = test_ds[['Encoded', 'Sequence_label', 'Key_label', 'URL']]\n",
    "val_data = val_ds[['Encoded', 'Sequence_label', 'Key_label', 'URL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1391559, 4)\n",
      "(624593, 4)\n",
      "(69400, 4)\n",
      "-------------\n",
      "(52033, 4)\n",
      "(572560, 4)\n",
      "-------------\n",
      "(5782, 4)\n",
      "(63618, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(val_data.shape)\n",
    "print('-------------')\n",
    "print(test_data[test_data['Sequence_label']==1].shape)\n",
    "print(test_data[test_data['Sequence_label']==0].shape)\n",
    "print('-------------')\n",
    "print(val_data[val_data['Sequence_label']==1].shape)\n",
    "print(val_data[val_data['Sequence_label']==0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ttqZYBggnVkU"
   },
   "outputs": [],
   "source": [
    "class LogDataset(Dataset):\n",
    "    def __init__(self, sequence, sequence_label, key_label, url):\n",
    "        self.sequence = sequence\n",
    "        self.sequence_label = sequence_label\n",
    "        self.key_label = key_label\n",
    "        self.url = url\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return (self.sequence[idx], self.sequence_label[idx], self.key_label[idx], self.url[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 512\n",
    "batch_size_test = 4096\n",
    "batch_size_val = 4096\n",
    "batch_size_train_test = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fLhwhscqnjOa"
   },
   "outputs": [],
   "source": [
    "setup_seed()\n",
    "\n",
    "def dataset_dataloader(data, batch_size):\n",
    "    sequence = data['Encoded'].tolist()\n",
    "    sequence_label = data['Sequence_label'].tolist()\n",
    "    key_label = data['Key_label'].tolist()\n",
    "    url = data['URL'].tolist()\n",
    "    dataset = LogDataset(sequence, sequence_label, key_label, url)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return data_loader\n",
    "\n",
    "train_loader = dataset_dataloader(train_data, batch_size = batch_size_train)\n",
    "test_loader = dataset_dataloader(test_data, batch_size = batch_size_test)\n",
    "val_loader = dataset_dataloader(val_data, batch_size = batch_size_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lf09uo30Z17N"
   },
   "source": [
    "**3. Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_mHXjMBWovar"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(dict_activity)\n",
    "embedding_dim = 50\n",
    "hidden_dim = 128\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "W64xqXJKoyJe"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim=8, hidden_dim=64, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.randn(self.num_layers, x.size(0), self.hidden_dim).cuda()\n",
    "        c0 = torch.randn(self.num_layers, x.size(0), self.hidden_dim).cuda()\n",
    "\n",
    "        embedded = self.embeddings(x)\n",
    "        out, (hidden, cell) = self.lstm(embedded, (h0, c0))    \n",
    "        return torch.squeeze(torch.mean(out, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3Q2cZYJOozqi"
   },
   "outputs": [],
   "source": [
    "model = Net(vocab_size, embedding_dim, hidden_dim, num_layers).cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKpZa23Ha0hK",
    "outputId": "2e1b0ad9-2db5-4357-e73b-1b0bd6a6d4b5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  MSE:  1.7111298796508443e-05\n",
      "Epoch  2  MSE:  1.4661513270013755e-07\n",
      "Epoch  3  MSE:  4.480076246219573e-08\n",
      "Epoch  4  MSE:  2.770248221289201e-08\n",
      "Epoch  5  MSE:  2.283668200128006e-08\n",
      "Epoch  6  MSE:  2.0344409000945383e-08\n",
      "Epoch  7  MSE:  1.992093370244203e-08\n",
      "Epoch  8  MSE:  1.76618581791396e-08\n",
      "Epoch  9  MSE:  1.796643341972515e-08\n",
      "Epoch  10  MSE:  1.724678771939636e-08\n",
      "Epoch  11  MSE:  1.668347685675328e-08\n",
      "Epoch  12  MSE:  1.6511824682211347e-08\n",
      "Epoch  13  MSE:  1.6129921775088583e-08\n",
      "Epoch  14  MSE:  1.6085868374362457e-08\n",
      "Epoch  15  MSE:  1.5602020245519646e-08\n",
      "Epoch  16  MSE:  1.5707807291513852e-08\n",
      "Epoch  17  MSE:  1.5700216238323977e-08\n",
      "Epoch  18  MSE:  1.455255947961226e-08\n",
      "Epoch  19  MSE:  1.6130051236001192e-08\n",
      "Epoch  20  MSE:  1.455249476819769e-08\n",
      "Epoch  21  MSE:  1.4607052468987215e-08\n",
      "Epoch  22  MSE:  1.4651550759166391e-08\n",
      "Epoch  23  MSE:  1.3830988360027054e-08\n",
      "Epoch  24  MSE:  1.4215134514184804e-08\n",
      "Epoch  25  MSE:  1.3916391651936955e-08\n",
      "Epoch  26  MSE:  1.40491030802659e-08\n",
      "Epoch  27  MSE:  1.3832618166947075e-08\n",
      "Epoch  28  MSE:  1.3503671173953628e-08\n",
      "Epoch  29  MSE:  1.366988014903224e-08\n",
      "Epoch  30  MSE:  1.346297875433202e-08\n",
      "Epoch  31  MSE:  1.3276345959275507e-08\n",
      "Epoch  32  MSE:  1.3292340558880122e-08\n",
      "Epoch  33  MSE:  1.3271424143846159e-08\n",
      "Epoch  34  MSE:  1.2926964841160516e-08\n",
      "Epoch  35  MSE:  1.2815079337813982e-08\n",
      "Epoch  36  MSE:  1.304091139694606e-08\n",
      "Epoch  37  MSE:  1.2560985683188904e-08\n",
      "Epoch  38  MSE:  1.265671063827495e-08\n",
      "Epoch  39  MSE:  1.2739617864578569e-08\n",
      "Epoch  40  MSE:  1.2431208091416061e-08\n",
      "Epoch  41  MSE:  1.250571904609545e-08\n",
      "Epoch  42  MSE:  1.2132407614078971e-08\n",
      "Epoch  43  MSE:  1.2496978885208265e-08\n",
      "Epoch  44  MSE:  1.2088354320043765e-08\n",
      "Epoch  45  MSE:  1.2257086493555783e-08\n",
      "Epoch  46  MSE:  1.2322657523789328e-08\n",
      "Epoch  47  MSE:  1.1630316122071196e-08\n",
      "Epoch  48  MSE:  1.1984214254142044e-08\n",
      "Epoch  49  MSE:  1.165265188709215e-08\n",
      "Epoch  50  MSE:  1.1856658778993553e-08\n"
     ]
    }
   ],
   "source": [
    "# if not os.path.exists('DeepSVDD.bin'):\n",
    "setup_seed()\n",
    "\n",
    "epochs = 50\n",
    "total_loss = []\n",
    "r_candidate = []\n",
    "min_loss = 10e6\n",
    "\n",
    "for i in range(epochs):\n",
    "    epoch_loss=[]\n",
    "    hidden_sum = torch.zeros((batch_size_train, hidden_dim))\n",
    "\n",
    "    if i < 20:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sequence, sequence_label, _, _ in train_loader:\n",
    "                if len(sequence_label) == batch_size_train:\n",
    "                    sequence = sequence.cuda()\n",
    "                    hidden_sum = hidden_sum.cuda()\n",
    "                    hidden1 = model(sequence)\n",
    "                    hidden_sum = hidden_sum + hidden1\n",
    "                    sequence = sequence.cpu()\n",
    "\n",
    "\n",
    "        center = (torch.mean(hidden_sum.cuda(), axis=0) / len(train_loader))\n",
    "        center_batch = torch.repeat_interleave(torch.unsqueeze(center, 0), batch_size_train, dim=0).detach()\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    for sequence2, sequence_label2, _, _ in train_loader:\n",
    "        if len(sequence_label2) == batch_size_train:\n",
    "            sequence2 = sequence2.cuda()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            hidden2 = model(sequence2)  \n",
    "            loss = criterion(hidden2, center_batch.cuda())  \n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "#             if i == epochs-1:\n",
    "#                 r_candidate.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Epoch \", i+1, \" MSE: \", np.mean(epoch_loss))\n",
    "    total_loss.append(np.max(epoch_loss))\n",
    "#         if total_loss[i] < min_loss:\n",
    "    if i==epochs-1:\n",
    "        torch.save(model.state_dict(), './DeepSVDD.bin')\n",
    "        min_loss = total_loss[i]\n",
    "        r = total_loss[i]\n",
    "\n",
    "        f = open('center_radius.txt', 'w+')\n",
    "        f.write(str(center.tolist()))\n",
    "        f.write('\\n')\n",
    "        f.write(str(r))\n",
    "        f.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bTmgJ_rXcVo9"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('DeepSVDD.bin'))\n",
    "\n",
    "f = open('center_radius.txt','r')\n",
    "center_radius = f.readlines()\n",
    "f.close()\n",
    "\n",
    "center = torch.tensor(eval(center_radius[0])).cuda()\n",
    "r = eval(center_radius[1])\n",
    "\n",
    "y_pred = []\n",
    "y_truth = []\n",
    "distance_list = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence, sequence_label, _, _ in val_loader: \n",
    "        y_truth = y_truth + sequence_label.tolist()\n",
    "\n",
    "        sequence = sequence.cuda()\n",
    "        hidden = model(sequence)\n",
    "        distance = torch.mean(torch.square(hidden-center), dim=1)\n",
    "        distance_list.extend(distance.tolist())\n",
    "        y_pred_batch = [int(i>r) for i in distance]\n",
    "        y_pred = y_pred + y_pred_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GK2i7KyGpHbf",
    "outputId": "1c339d60-37dd-494c-a884-d9363f9e498f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9681    1.0000    0.9838     63618\n",
      "           1     1.0000    0.6377    0.7788      5782\n",
      "\n",
      "    accuracy                         0.9698     69400\n",
      "   macro avg     0.9841    0.8188    0.8813     69400\n",
      "weighted avg     0.9708    0.9698    0.9667     69400\n",
      "\n",
      "[[63618     0]\n",
      " [ 2095  3687]]\n",
      "0.8188343133863715\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_truth, y_pred, digits=4))\n",
    "print(metrics.confusion_matrix(y_truth, y_pred))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_truth, y_pred, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))\n",
    "\n",
    "f = open('output.txt', 'a')\n",
    "f.write('Sequence anomaly detection: '+'\\n')\n",
    "f.write(str(metrics.classification_report(y_truth, y_pred, digits=4))+'\\n')\n",
    "f.write(str(metrics.confusion_matrix(y_truth, y_pred))+'\\n')\n",
    "f.write(str(metrics.auc(fpr, tpr))+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('DeepSVDD.bin'))\n",
    "\n",
    "f = open('center_radius.txt','r')\n",
    "center_radius = f.readlines()\n",
    "f.close()\n",
    "\n",
    "center = torch.tensor(eval(center_radius[0])).cuda()\n",
    "r = eval(center_radius[1])\n",
    "\n",
    "y_pred = []\n",
    "y_truth = []\n",
    "seq_list = []\n",
    "distance_list = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence, sequence_label, _, _ in train_loader: \n",
    "        y_truth = y_truth + sequence_label.tolist()\n",
    "        seq_list += sequence.tolist()\n",
    "        sequence = sequence.cuda()\n",
    "        hidden = model(sequence)\n",
    "        distance = torch.mean(torch.square(hidden-center), dim=1)\n",
    "        distance_list += distance.tolist()\n",
    "        y_pred_batch = [int(i>r) for i in distance]\n",
    "        y_pred = y_pred + y_pred_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7, 4, 9, 9, 5, 7, 4, 7, 9, 9, 9, 9, 9, 9, 7, 9, 9, 9, 9],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_sequence = torch.tensor(seq_list[np.argmin(distance_list)]).to(device)\n",
    "baseline_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GSbFooWvkWBK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sequence_list = []\n",
    "sequence_label_list = []\n",
    "key_label_list = []\n",
    "url_list = []\n",
    "\n",
    "sequence_list2 = []\n",
    "sequence_label_list2 = []\n",
    "key_label_list2 = []\n",
    "url_list2 = []\n",
    "\n",
    "sequence_list3 = []\n",
    "sequence_label_list3 = []\n",
    "key_label_list3 = []\n",
    "url_list3 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence, sequence_label, key_label, url in test_loader: \n",
    "        sequence = sequence.cuda()\n",
    "        \n",
    "        hidden = model(sequence)\n",
    "        distance = torch.mean(torch.square(hidden-center), dim=1)\n",
    "        y_pred_index_batch = [i for i in range(len(distance)) if distance[i]>10*r]\n",
    "        y_pred_index_batch2 = [i for i in range(len(distance)) if distance[i]>r]\n",
    "        y_pred_index_batch3 = [i for i in range(len(distance)) if distance[i]<=r]\n",
    "        \n",
    "        sequence_l = sequence.tolist()\n",
    "        sequence_label_l = sequence_label.tolist()\n",
    "        key_label_l = key_label.tolist()\n",
    "        url_l = url.tolist()\n",
    "        \n",
    "        for i in y_pred_index_batch:\n",
    "            sequence_list += [sequence_l[i]]\n",
    "            sequence_label_list += [sequence_label_l[i]]\n",
    "            key_label_list += [key_label_l[i]]\n",
    "            url_list += [url_l[i]]\n",
    "            \n",
    "        for j in y_pred_index_batch2:\n",
    "            sequence_list2 += [sequence_l[j]]\n",
    "            sequence_label_list2 += [sequence_label_l[j]]\n",
    "            key_label_list2 += [key_label_l[j]]\n",
    "            url_list2 += [url_l[j]]\n",
    "            \n",
    "        for k in y_pred_index_batch3:\n",
    "            sequence_list3 += [sequence_l[k]]\n",
    "            sequence_label_list3 += [sequence_label_l[k]]\n",
    "            key_label_list3 += [key_label_l[k]]\n",
    "            url_list3 += [url_l[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "aalNggSKkWBK"
   },
   "outputs": [],
   "source": [
    "def train_test_data_loader(sequence_list, sequence_label_list, key_label_list, url_list):\n",
    "    d = {'Encoded': sequence_list,\n",
    "         'Sequence_label': sequence_label_list,\n",
    "         'Key_label': key_label_list,\n",
    "         'URL': url_list}\n",
    "\n",
    "    train_test_data = pd.DataFrame(d)\n",
    "\n",
    "    train_test_data['Encoded'] = [torch.tensor(i) for i in train_test_data['Encoded']]\n",
    "    train_test_data['Sequence_label'] = [torch.tensor(i) for i in train_test_data['Sequence_label']]\n",
    "    train_test_data['Key_label'] = [torch.tensor(i) for i in train_test_data['Key_label']]\n",
    "    train_test_data['URL'] = [torch.tensor(i) for i in train_test_data['URL']]\n",
    "\n",
    "    train_test_loader = dataset_dataloader(train_test_data, batch_size = batch_size_train_test)\n",
    "    return train_test_loader, train_test_data\n",
    "\n",
    "train_test_loader, train_test_data   = train_test_data_loader(sequence_list, sequence_label_list, key_label_list, url_list)\n",
    "train_test_loader2, train_test_data2 = train_test_data_loader(sequence_list2, sequence_label_list2, key_label_list2, url_list2)\n",
    "train_test_loader3, train_test_data3 = train_test_data_loader(sequence_list3, sequence_label_list3, key_label_list3, url_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Encoded</th>\n",
       "      <th>Sequence_label</th>\n",
       "      <th>Key_label</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[tensor(7), tensor(7), tensor(7), tensor(7), t...</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(24), tensor(53), tensor(49), tensor(19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[tensor(7), tensor(7), tensor(7), tensor(7), t...</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(53), tensor(69), tensor(295), tensor(8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[tensor(7), tensor(7), tensor(6), tensor(6), t...</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(36), tensor(296), tensor(2), tensor(2)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[tensor(7), tensor(7), tensor(7), tensor(7), t...</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(64), tensor(39), tensor(64), tensor(39...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[tensor(6), tensor(6), tensor(7), tensor(7), t...</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(2), tensor(2), tensor(287), tensor(391...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590925</th>\n",
       "      <td>[tensor(5), tensor(7), tensor(4), tensor(5), t...</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>[tensor(1), tensor(0), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(2), tensor(63), tensor(2), tensor(2), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590987</th>\n",
       "      <td>[tensor(7), tensor(5), tensor(7), tensor(4), t...</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>[tensor(0), tensor(1), tensor(0), tensor(1), t...</td>\n",
       "      <td>[tensor(369), tensor(2), tensor(22), tensor(2)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591002</th>\n",
       "      <td>[tensor(9), tensor(9), tensor(5), tensor(4), t...</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(2), tensor(2), tensor(2), tensor(2), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591012</th>\n",
       "      <td>[tensor(4), tensor(7), tensor(7), tensor(7), t...</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>[tensor(1), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(2), tensor(32), tensor(218), tensor(19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591040</th>\n",
       "      <td>[tensor(7), tensor(7), tensor(7), tensor(7), t...</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(36), tensor(36), tensor(36), tensor(36...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18496 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Encoded Sequence_label  \\\n",
       "7       [tensor(7), tensor(7), tensor(7), tensor(7), t...      tensor(1)   \n",
       "16      [tensor(7), tensor(7), tensor(7), tensor(7), t...      tensor(1)   \n",
       "40      [tensor(7), tensor(7), tensor(6), tensor(6), t...      tensor(1)   \n",
       "41      [tensor(7), tensor(7), tensor(7), tensor(7), t...      tensor(1)   \n",
       "49      [tensor(6), tensor(6), tensor(7), tensor(7), t...      tensor(1)   \n",
       "...                                                   ...            ...   \n",
       "590925  [tensor(5), tensor(7), tensor(4), tensor(5), t...      tensor(1)   \n",
       "590987  [tensor(7), tensor(5), tensor(7), tensor(4), t...      tensor(1)   \n",
       "591002  [tensor(9), tensor(9), tensor(5), tensor(4), t...      tensor(1)   \n",
       "591012  [tensor(4), tensor(7), tensor(7), tensor(7), t...      tensor(1)   \n",
       "591040  [tensor(7), tensor(7), tensor(7), tensor(7), t...      tensor(1)   \n",
       "\n",
       "                                                Key_label  \\\n",
       "7       [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "16      [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "40      [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "41      [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "49      [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "...                                                   ...   \n",
       "590925  [tensor(1), tensor(0), tensor(1), tensor(1), t...   \n",
       "590987  [tensor(0), tensor(1), tensor(0), tensor(1), t...   \n",
       "591002  [tensor(0), tensor(0), tensor(1), tensor(1), t...   \n",
       "591012  [tensor(1), tensor(0), tensor(0), tensor(0), t...   \n",
       "591040  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "\n",
       "                                                      URL  \n",
       "7       [tensor(24), tensor(53), tensor(49), tensor(19...  \n",
       "16      [tensor(53), tensor(69), tensor(295), tensor(8...  \n",
       "40      [tensor(36), tensor(296), tensor(2), tensor(2)...  \n",
       "41      [tensor(64), tensor(39), tensor(64), tensor(39...  \n",
       "49      [tensor(2), tensor(2), tensor(287), tensor(391...  \n",
       "...                                                   ...  \n",
       "590925  [tensor(2), tensor(63), tensor(2), tensor(2), ...  \n",
       "590987  [tensor(369), tensor(2), tensor(22), tensor(2)...  \n",
       "591002  [tensor(2), tensor(2), tensor(2), tensor(2), t...  \n",
       "591012  [tensor(2), tensor(32), tensor(218), tensor(19...  \n",
       "591040  [tensor(36), tensor(36), tensor(36), tensor(36...  \n",
       "\n",
       "[18496 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_data3[train_test_data3['Sequence_label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "wWxEAjsykWBK"
   },
   "outputs": [],
   "source": [
    "embedding_dim2 = 100\n",
    "hidden_dim2 = 128\n",
    "num_layers2 = 1\n",
    "triplet_lambda = 1\n",
    "continuity_lambda = 0.15\n",
    "sparsity_lambda = 0.1\n",
    "epochs2 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=256, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embeddings(x)\n",
    "        out, (hidden, cell) = self.lstm(embedded)    \n",
    "        scores = self.output_layer(out) \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "D93Iozz-kWBL"
   },
   "outputs": [],
   "source": [
    "class CFDet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CFDet, self).__init__()\n",
    "        self.exploration_rate = 0.05\n",
    "        self.count_tokens = 2\n",
    "        self.count_pieces = 2\n",
    "        self.generator = Generator(vocab_size, embedding_dim2, hidden_dim2, num_layers2).cuda()\n",
    "\n",
    "    def generate(self, x, training=True):\n",
    "        z_scores_ = self.generator(x)\n",
    "        z_probs_ = F.softmax(z_scores_, dim=-1)\n",
    "        z_prob_ = (1 - self.exploration_rate) * z_probs_ + self.exploration_rate / z_probs_.size(-1)\n",
    "        z_prob__ = z_prob_.view(-1, 2)\n",
    "        sampler = torch.distributions.Categorical(z_prob__)\n",
    "\n",
    "        if training:\n",
    "            z_ = sampler.sample()  # (num_rows * p_length,)\n",
    "            z = z_.view(z_prob_.size(0), z_prob_.size(1))\n",
    "            z = z.type(torch.cuda.IntTensor)\n",
    "            neg_log_probs_ = -sampler.log_prob(z_)\n",
    "            neg_log_probs = neg_log_probs_.view(z_prob_.size(0), z_prob_.size(1))\n",
    "            return z, neg_log_probs\n",
    "        else:\n",
    "            z__index = torch.max(z_prob__, dim=-1)[1]\n",
    "            z0 = z__index.view(z_prob_.size(0), z_prob_.size(1))\n",
    "            z_index = z0.type(torch.cuda.IntTensor)\n",
    "\n",
    "            z__value = torch.max(z_prob__, dim=-1)[0]\n",
    "            # z1 = z__value.view(z_prob_.size(0), z_prob_.size(1))\n",
    "            z_value = z__value.type(torch.cuda.FloatTensor)\n",
    "            return z_index, z_value\n",
    "\n",
    "    def get_loss(self, x, z, neg_log_probs,average_reward, batch_size, model, sequence_length=20.0):\n",
    "        z_ = torch.cat([z[:, 1:], z[:, -1:]], dim=-1)\n",
    "        continuity_ratio = torch.div(torch.sum(torch.abs(z - z_), dim=-1), sequence_length)\n",
    "        percentage = (self.count_pieces-1) / sequence_length\n",
    "        continuity_loss = torch.abs(continuity_ratio - percentage)\n",
    "#         continuity_loss = torch.clamp(continuity_ratio - percentage, min=0)\n",
    "\n",
    "\n",
    "        sparsity_ratio = torch.div(torch.sum(z, dim=-1), sequence_length)\n",
    "        percentage = self.count_tokens / sequence_length\n",
    "        sparsity_loss = torch.abs(sparsity_ratio - percentage)\n",
    "#         sparsity_loss = torch.clamp(sparsity_ratio - percentage, min=0)\n",
    "\n",
    "        anomalous_entry = x * z + baseline_sequence * (1-z)\n",
    "        anti = x * (1-z) + baseline_sequence * z\n",
    "        hidden_anomalous_entry = model(anomalous_entry)\n",
    "        hidden_anti = model(anti)\n",
    "        distance_loss = criterion2(center_batch2, hidden_anti, hidden_anomalous_entry) + criterion(center_batch2, hidden_anti) \\\n",
    "                        - criterion(center_batch2, hidden_anomalous_entry)\n",
    "\n",
    "        average_reward = average_reward.cuda()\n",
    "        rewards = -(triplet_lambda * distance_loss + sparsity_lambda * sparsity_loss + continuity_lambda * continuity_loss ).detach()\n",
    "        advantages = rewards - average_reward # (batch_size,)\n",
    "\n",
    "        advantages_expand_ = advantages.unsqueeze(-1).expand_as(neg_log_probs)       \n",
    "        rl_loss = torch.sum(neg_log_probs * advantages_expand_)\n",
    "        \n",
    "        return distance_loss, rl_loss, rewards, continuity_loss, sparsity_loss, advantages_expand_\n",
    "\n",
    "    def training_step(self, distance_loss, rl_loss):\n",
    "        rl_loss.backward()\n",
    "        optimiser2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Hj_YWgagkWBL"
   },
   "outputs": [],
   "source": [
    "cfdet = CFDet()\n",
    "criterion2 = nn.TripletMarginLoss(margin=1, reduction='none')\n",
    "optimiser2 = optim.Adam(cfdet.generator.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z859sJLQkWBM",
    "outputId": "46554397-351f-4c88-830c-b5e5cfba80c5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1:\n",
      "distance_loss: 0.869846798479557 continuity loss:  0.22965087764896452 sparsity loss:  0.670484914444387\n",
      "------------------------------------------------------\n",
      "epoch2:\n",
      "distance_loss: 0.8371458407491446 continuity loss:  0.05748443747870624 sparsity loss:  0.8566604480147362\n",
      "------------------------------------------------------\n",
      "epoch3:\n",
      "distance_loss: 0.8372033704072237 continuity loss:  0.05817260907497257 sparsity loss:  0.8574325405061245\n",
      "------------------------------------------------------\n",
      "epoch4:\n",
      "distance_loss: 0.8368851561099291 continuity loss:  0.06029205513186753 sparsity loss:  0.8480041287839413\n",
      "------------------------------------------------------\n",
      "epoch5:\n",
      "distance_loss: 0.8379889521747828 continuity loss:  0.06542053434532136 sparsity loss:  0.7795226946473122\n",
      "------------------------------------------------------\n",
      "epoch6:\n",
      "distance_loss: 0.8421878442168236 continuity loss:  0.14727783389389515 sparsity loss:  0.11430359119549394\n",
      "------------------------------------------------------\n",
      "epoch7:\n",
      "distance_loss: 0.839969927445054 continuity loss:  0.14471283182501793 sparsity loss:  0.0660644571762532\n",
      "------------------------------------------------------\n",
      "epoch8:\n",
      "distance_loss: 0.8394006229937077 continuity loss:  0.14356079557910562 sparsity loss:  0.06401977874338627\n",
      "------------------------------------------------------\n",
      "epoch9:\n",
      "distance_loss: 0.8396056685596704 continuity loss:  0.1429443396627903 sparsity loss:  0.06643829587846994\n",
      "------------------------------------------------------\n",
      "epoch10:\n",
      "distance_loss: 0.8396962806582451 continuity loss:  0.1419662511907518 sparsity loss:  0.06970672938041389\n",
      "------------------------------------------------------\n",
      "epoch11:\n",
      "distance_loss: 0.8395350128412247 continuity loss:  0.14197082817554474 sparsity loss:  0.06978302309289575\n",
      "------------------------------------------------------\n",
      "epoch12:\n",
      "distance_loss: 0.8398149218410254 continuity loss:  0.14089508447796106 sparsity loss:  0.06714477844070643\n",
      "------------------------------------------------------\n",
      "epoch13:\n",
      "distance_loss: 0.8393966723233461 continuity loss:  0.1420242334716022 sparsity loss:  0.06201629899442196\n",
      "------------------------------------------------------\n",
      "epoch14:\n",
      "distance_loss: 0.8394287712872028 continuity loss:  0.14250183291733265 sparsity loss:  0.06064911151770502\n",
      "------------------------------------------------------\n",
      "epoch15:\n",
      "distance_loss: 0.839790802448988 continuity loss:  0.14142761612311006 sparsity loss:  0.06421966885682195\n",
      "------------------------------------------------------\n",
      "epoch16:\n",
      "distance_loss: 0.8395150620490313 continuity loss:  0.1421707165427506 sparsity loss:  0.06416168517898768\n",
      "------------------------------------------------------\n",
      "epoch17:\n",
      "distance_loss: 0.8392796441912651 continuity loss:  0.1419281018897891 sparsity loss:  0.06437378271948546\n",
      "------------------------------------------------------\n",
      "epoch18:\n",
      "distance_loss: 0.8395341411232948 continuity loss:  0.14195556845515966 sparsity loss:  0.06278991943690926\n",
      "------------------------------------------------------\n",
      "epoch19:\n",
      "distance_loss: 0.8394833486527205 continuity loss:  0.14112549182027578 sparsity loss:  0.065412905998528\n",
      "------------------------------------------------------\n",
      "epoch20:\n",
      "distance_loss: 0.8397736046463251 continuity loss:  0.14053802844136953 sparsity loss:  0.06991882738657296\n",
      "------------------------------------------------------\n",
      "epoch21:\n",
      "distance_loss: 0.8394626602530479 continuity loss:  0.1401138324290514 sparsity loss:  0.06978912628255785\n",
      "------------------------------------------------------\n",
      "epoch22:\n",
      "distance_loss: 0.8396730776876211 continuity loss:  0.14100952446460724 sparsity loss:  0.06859436235390604\n",
      "------------------------------------------------------\n",
      "epoch23:\n",
      "distance_loss: 0.839551942422986 continuity loss:  0.14047546917572618 sparsity loss:  0.06970367743633687\n",
      "------------------------------------------------------\n",
      "epoch24:\n",
      "distance_loss: 0.8400428276509047 continuity loss:  0.14007721096277237 sparsity loss:  0.06960907264146954\n",
      "------------------------------------------------------\n",
      "epoch25:\n",
      "distance_loss: 0.8390357196331024 continuity loss:  0.14081726362928748 sparsity loss:  0.06652832322288305\n",
      "------------------------------------------------------\n",
      "epoch26:\n",
      "distance_loss: 0.8393926918506622 continuity loss:  0.14086761884391308 sparsity loss:  0.06589203211478889\n",
      "------------------------------------------------------\n",
      "epoch27:\n",
      "distance_loss: 0.8395854718983173 continuity loss:  0.14139709854498506 sparsity loss:  0.06500244431663305\n",
      "------------------------------------------------------\n",
      "epoch28:\n",
      "distance_loss: 0.8397193588316441 continuity loss:  0.14177551586180925 sparsity loss:  0.0626709011849016\n",
      "------------------------------------------------------\n",
      "epoch29:\n",
      "distance_loss: 0.8392164930701256 continuity loss:  0.14214477967470884 sparsity loss:  0.05948486574925482\n",
      "------------------------------------------------------\n",
      "epoch30:\n",
      "distance_loss: 0.8394267428666353 continuity loss:  0.14273986965417862 sparsity loss:  0.06075592280831188\n",
      "------------------------------------------------------\n",
      "epoch31:\n",
      "distance_loss: 0.839781267568469 continuity loss:  0.14218140067532659 sparsity loss:  0.061436465941369534\n",
      "------------------------------------------------------\n",
      "epoch32:\n",
      "distance_loss: 0.8391006626188755 continuity loss:  0.14236145140603185 sparsity loss:  0.061924746609292924\n",
      "------------------------------------------------------\n",
      "epoch33:\n",
      "distance_loss: 0.8394816592335701 continuity loss:  0.1417663600295782 sparsity loss:  0.06269684131257236\n",
      "------------------------------------------------------\n",
      "epoch34:\n",
      "distance_loss: 0.8395225629210472 continuity loss:  0.14132995950058103 sparsity loss:  0.06480407959315926\n",
      "------------------------------------------------------\n",
      "epoch35:\n",
      "distance_loss: 0.8395765945315361 continuity loss:  0.14083252102136612 sparsity loss:  0.06691894866526127\n",
      "------------------------------------------------------\n",
      "epoch36:\n",
      "distance_loss: 0.8395310137420893 continuity loss:  0.14065399672836065 sparsity loss:  0.06469269096851349\n",
      "------------------------------------------------------\n",
      "epoch37:\n",
      "distance_loss: 0.8397729471325874 continuity loss:  0.1419052160345018 sparsity loss:  0.06326599360909313\n",
      "------------------------------------------------------\n",
      "epoch38:\n",
      "distance_loss: 0.839379720389843 continuity loss:  0.1414947547018528 sparsity loss:  0.06590576376765966\n",
      "------------------------------------------------------\n",
      "epoch39:\n",
      "distance_loss: 0.8397173546254635 continuity loss:  0.1404754682444036 sparsity loss:  0.06676483538467437\n",
      "------------------------------------------------------\n",
      "epoch40:\n",
      "distance_loss: 0.8391029387712479 continuity loss:  0.14085235772654414 sparsity loss:  0.06648407282773405\n",
      "------------------------------------------------------\n",
      "epoch41:\n",
      "distance_loss: 0.8393425941467285 continuity loss:  0.14053802471607924 sparsity loss:  0.06554413202684373\n",
      "------------------------------------------------------\n",
      "epoch42:\n",
      "distance_loss: 0.8394708465784788 continuity loss:  0.14135284628719091 sparsity loss:  0.06574249535333365\n",
      "------------------------------------------------------\n",
      "epoch43:\n",
      "distance_loss: 0.8391311950981617 continuity loss:  0.14147186651825905 sparsity loss:  0.06444244703743607\n",
      "------------------------------------------------------\n",
      "epoch44:\n",
      "distance_loss: 0.8394398298114538 continuity loss:  0.1408569375053048 sparsity loss:  0.06472778564784676\n",
      "------------------------------------------------------\n",
      "epoch45:\n",
      "distance_loss: 0.8397475928068161 continuity loss:  0.14089660998433828 sparsity loss:  0.06340027111582458\n",
      "------------------------------------------------------\n",
      "epoch46:\n",
      "distance_loss: 0.8393552117049694 continuity loss:  0.14209595136344433 sparsity loss:  0.06161804462317377\n",
      "------------------------------------------------------\n",
      "epoch47:\n",
      "distance_loss: 0.8395682591944933 continuity loss:  0.14100799802690744 sparsity loss:  0.0633590737124905\n",
      "------------------------------------------------------\n",
      "epoch48:\n",
      "distance_loss: 0.8393208477646112 continuity loss:  0.1411788947880268 sparsity loss:  0.06562347617000341\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch49:\n",
      "distance_loss: 0.8391740974038839 continuity loss:  0.14129333849996328 sparsity loss:  0.06524811103008687\n",
      "------------------------------------------------------\n",
      "epoch50:\n",
      "distance_loss: 0.8395763840526342 continuity loss:  0.1412033117376268 sparsity loss:  0.0647201573010534\n",
      "------------------------------------------------------\n",
      "epoch51:\n",
      "distance_loss: 0.8393663913011551 continuity loss:  0.14090271154418588 sparsity loss:  0.0655761748785153\n",
      "------------------------------------------------------\n",
      "epoch52:\n",
      "distance_loss: 0.839734872803092 continuity loss:  0.13994751032441854 sparsity loss:  0.06559143401682377\n",
      "------------------------------------------------------\n",
      "epoch53:\n",
      "distance_loss: 0.8395945373922586 continuity loss:  0.1407730132341385 sparsity loss:  0.06387634587008506\n",
      "------------------------------------------------------\n",
      "epoch54:\n",
      "distance_loss: 0.8394881468266249 continuity loss:  0.14150543650612235 sparsity loss:  0.06302643159870058\n",
      "------------------------------------------------------\n",
      "epoch55:\n",
      "distance_loss: 0.839736882597208 continuity loss:  0.14213104592636228 sparsity loss:  0.06188354769255966\n",
      "------------------------------------------------------\n",
      "epoch56:\n",
      "distance_loss: 0.8396365623921156 continuity loss:  0.14306793361902237 sparsity loss:  0.0576019313884899\n",
      "------------------------------------------------------\n",
      "epoch57:\n",
      "distance_loss: 0.8393728397786617 continuity loss:  0.14310608291998506 sparsity loss:  0.0579376247478649\n",
      "------------------------------------------------------\n",
      "epoch58:\n",
      "distance_loss: 0.8398962076753378 continuity loss:  0.14173584012314677 sparsity loss:  0.059371951734647155\n",
      "------------------------------------------------------\n",
      "epoch59:\n",
      "distance_loss: 0.8392230644822121 continuity loss:  0.1419052160345018 sparsity loss:  0.060441592475399375\n",
      "------------------------------------------------------\n",
      "epoch60:\n",
      "distance_loss: 0.8394389934837818 continuity loss:  0.14105224749073386 sparsity loss:  0.05960846249945462\n",
      "------------------------------------------------------\n",
      "epoch61:\n",
      "distance_loss: 0.8399277534335852 continuity loss:  0.1415023822337389 sparsity loss:  0.05969238583929837\n",
      "------------------------------------------------------\n",
      "epoch62:\n",
      "distance_loss: 0.8390847574919462 continuity loss:  0.1421493561938405 sparsity loss:  0.06006927788257599\n",
      "------------------------------------------------------\n",
      "epoch63:\n",
      "distance_loss: 0.8392061050981283 continuity loss:  0.14227447798475623 sparsity loss:  0.060330202570185065\n",
      "------------------------------------------------------\n",
      "epoch64:\n",
      "distance_loss: 0.8395891413092613 continuity loss:  0.14188690297305584 sparsity loss:  0.060734560480341315\n",
      "------------------------------------------------------\n",
      "epoch65:\n",
      "distance_loss: 0.8393872119486332 continuity loss:  0.14215393410995603 sparsity loss:  0.06062622368335724\n",
      "------------------------------------------------------\n",
      "epoch66:\n",
      "distance_loss: 0.8394638150930405 continuity loss:  0.14275055145844817 sparsity loss:  0.06064758589491248\n",
      "------------------------------------------------------\n",
      "epoch67:\n",
      "distance_loss: 0.8393968343734741 continuity loss:  0.14243164379149675 sparsity loss:  0.06043701455928385\n",
      "------------------------------------------------------\n",
      "epoch68:\n",
      "distance_loss: 0.8388055041432381 continuity loss:  0.14157562470063567 sparsity loss:  0.059747318155132234\n",
      "------------------------------------------------------\n",
      "epoch69:\n",
      "distance_loss: 0.839440057054162 continuity loss:  0.14178314479067922 sparsity loss:  0.0593109157634899\n",
      "------------------------------------------------------\n",
      "epoch70:\n",
      "distance_loss: 0.8389658499509096 continuity loss:  0.14306030655279756 sparsity loss:  0.05746460217051208\n",
      "------------------------------------------------------\n",
      "epoch71:\n",
      "distance_loss: 0.8392799813300371 continuity loss:  0.14322357345372438 sparsity loss:  0.056137087172828615\n",
      "------------------------------------------------------\n",
      "epoch72:\n",
      "distance_loss: 0.8393245972692966 continuity loss:  0.1427597077563405 sparsity loss:  0.056423953268676996\n",
      "------------------------------------------------------\n",
      "epoch73:\n",
      "distance_loss: 0.8394592497497797 continuity loss:  0.14352417271584272 sparsity loss:  0.05649261700455099\n",
      "------------------------------------------------------\n",
      "epoch74:\n",
      "distance_loss: 0.8394564036279917 continuity loss:  0.14328460860997438 sparsity loss:  0.05607910442631692\n",
      "------------------------------------------------------\n",
      "epoch75:\n",
      "distance_loss: 0.8398397602140903 continuity loss:  0.1442123418673873 sparsity loss:  0.05423279106616974\n",
      "------------------------------------------------------\n",
      "epoch76:\n",
      "distance_loss: 0.8399016410112381 continuity loss:  0.14399872161448002 sparsity loss:  0.05395202920772135\n",
      "------------------------------------------------------\n",
      "epoch77:\n",
      "distance_loss: 0.8397432956844568 continuity loss:  0.14405060280114412 sparsity loss:  0.05367431859485805\n",
      "------------------------------------------------------\n",
      "epoch78:\n",
      "distance_loss: 0.839625746011734 continuity loss:  0.14483185159042478 sparsity loss:  0.05389862309675664\n",
      "------------------------------------------------------\n",
      "epoch79:\n",
      "distance_loss: 0.8392491359263659 continuity loss:  0.14427643083035946 sparsity loss:  0.053666689433157444\n",
      "------------------------------------------------------\n",
      "epoch80:\n",
      "distance_loss: 0.8394614513963461 continuity loss:  0.14431305043399334 sparsity loss:  0.05347442894708365\n",
      "------------------------------------------------------\n",
      "epoch81:\n",
      "distance_loss: 0.8395499717444181 continuity loss:  0.14413299970328808 sparsity loss:  0.053526308853179216\n",
      "------------------------------------------------------\n",
      "epoch82:\n",
      "distance_loss: 0.8396832905709743 continuity loss:  0.14413299690932035 sparsity loss:  0.05372314772102982\n",
      "------------------------------------------------------\n",
      "epoch83:\n",
      "distance_loss: 0.8394518680870533 continuity loss:  0.14334106724709272 sparsity loss:  0.05351410142611712\n",
      "------------------------------------------------------\n",
      "epoch84:\n",
      "distance_loss: 0.8394923768937588 continuity loss:  0.143981936853379 sparsity loss:  0.05358276574406773\n",
      "------------------------------------------------------\n",
      "epoch85:\n",
      "distance_loss: 0.8393506649881601 continuity loss:  0.1445556664839387 sparsity loss:  0.05351104971487075\n",
      "------------------------------------------------------\n",
      "epoch86:\n",
      "distance_loss: 0.8395986892282963 continuity loss:  0.14467315841466188 sparsity loss:  0.05375976790674031\n",
      "------------------------------------------------------\n",
      "epoch87:\n",
      "distance_loss: 0.8395519107580185 continuity loss:  0.144297793507576 sparsity loss:  0.053767397184856236\n",
      "------------------------------------------------------\n",
      "epoch88:\n",
      "distance_loss: 0.8399246819317341 continuity loss:  0.14399719657376409 sparsity loss:  0.05347290320787579\n",
      "------------------------------------------------------\n",
      "epoch89:\n",
      "distance_loss: 0.8395792711526155 continuity loss:  0.14388428023084998 sparsity loss:  0.05357666255440563\n",
      "------------------------------------------------------\n",
      "epoch90:\n",
      "distance_loss: 0.8399288691580296 continuity loss:  0.14429321512579918 sparsity loss:  0.05336303939111531\n",
      "------------------------------------------------------\n",
      "epoch91:\n",
      "distance_loss: 0.8392518479377031 continuity loss:  0.14423675742000341 sparsity loss:  0.053688051528297365\n",
      "------------------------------------------------------\n",
      "epoch92:\n",
      "distance_loss: 0.8394562341272831 continuity loss:  0.1436065686866641 sparsity loss:  0.05322723661083728\n",
      "------------------------------------------------------\n",
      "epoch93:\n",
      "distance_loss: 0.8397669047117233 continuity loss:  0.14406280731782317 sparsity loss:  0.0538650534581393\n",
      "------------------------------------------------------\n",
      "epoch94:\n",
      "distance_loss: 0.8396546877920628 continuity loss:  0.14464874332770705 sparsity loss:  0.053680422133766115\n",
      "------------------------------------------------------\n",
      "epoch95:\n",
      "distance_loss: 0.8394189216196537 continuity loss:  0.14414215413853526 sparsity loss:  0.053422548808157444\n",
      "------------------------------------------------------\n",
      "epoch96:\n",
      "distance_loss: 0.8398373126983643 continuity loss:  0.14410858415067196 sparsity loss:  0.05359192134346813\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch97:\n",
      "distance_loss: 0.8394320867955685 continuity loss:  0.14402466267347336 sparsity loss:  0.053797915345057845\n",
      "------------------------------------------------------\n",
      "epoch98:\n",
      "distance_loss: 0.8396264277398586 continuity loss:  0.14415435958653688 sparsity loss:  0.05343017855193466\n",
      "------------------------------------------------------\n",
      "epoch99:\n",
      "distance_loss: 0.8391272984445095 continuity loss:  0.14418182661756873 sparsity loss:  0.05382995854597539\n",
      "------------------------------------------------------\n",
      "epoch100:\n",
      "distance_loss: 0.8396367207169533 continuity loss:  0.14379425207152963 sparsity loss:  0.05326233129017055\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# if not os.path.exists('state_dict_minloss.bin'):\n",
    "\n",
    "setup_seed()\n",
    "\n",
    "total_loss_list = []\n",
    "distance_loss_list = []\n",
    "reward_list = []\n",
    "continuity_loss_list = []\n",
    "sparsity_loss_list = []\n",
    "loss_list = []\n",
    "\n",
    "min_loss= 10e6\n",
    "\n",
    "center_batch2 = torch.repeat_interleave(torch.unsqueeze(center, 0), batch_size_train_test, dim=0).cuda()\n",
    "\n",
    "for i in range(epochs2):\n",
    "    z_history_rewards = deque(maxlen=200)\n",
    "    z_history_rewards.append(0.0)\n",
    "    epoch_distance_loss = []\n",
    "    epoch_continuity_loss = []\n",
    "    epoch_sparsity_loss = []\n",
    "    epoch_rl_loss = []\n",
    "    epoch_reward = []\n",
    "    epoch_loss= []\n",
    "\n",
    "    cfdet.generator.train()\n",
    "    model.train()\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False \n",
    "\n",
    "    for sequence4, sequence_label4, _, _ in train_test_loader:\n",
    "        sequence4 = sequence4.cuda()\n",
    "\n",
    "        baseline = Variable(torch.FloatTensor([float(np.mean(z_history_rewards))]))\n",
    "\n",
    "        if len(sequence_label4) == batch_size_train_test:             \n",
    "            optimiser2.zero_grad()\n",
    "\n",
    "            z, neg_log_probs = cfdet.generate(sequence4)\n",
    "            distance_loss, rl_loss, rewards, continuity_loss, sparsity_loss, advantage = cfdet.get_loss(sequence4, z, neg_log_probs, baseline, batch_size_train_test, model)\n",
    "            cfdet.training_step(distance_loss, rl_loss)\n",
    "\n",
    "            epoch_distance_loss.append(torch.mean(distance_loss).item())\n",
    "            epoch_continuity_loss.append(torch.mean(continuity_loss).item())\n",
    "            epoch_sparsity_loss.append(torch.mean(sparsity_loss).item())\n",
    "            epoch_rl_loss.append(rl_loss.item())\n",
    "            epoch_reward.append(torch.sum(rewards).item())\n",
    "            epoch_loss.append(torch.sum(-rewards).item())\n",
    "\n",
    "            z_batch_reward = np.mean(rewards.cpu().data.numpy())\n",
    "            z_history_rewards.append(z_batch_reward)\n",
    "\n",
    "    total_loss_list.append(np.mean(epoch_rl_loss))\n",
    "    continuity_loss_list.append(np.mean(epoch_continuity_loss))\n",
    "    sparsity_loss_list.append(np.mean(epoch_sparsity_loss))\n",
    "    distance_loss_list.append(np.mean(epoch_distance_loss))\n",
    "    reward_list.append(np.mean(epoch_reward))\n",
    "    loss_list.append(np.mean(epoch_loss))\n",
    "\n",
    "    if distance_loss_list[i] + continuity_lambda * continuity_loss_list[i] + sparsity_lambda * sparsity_loss_list[i] < min_loss:\n",
    "        min_loss = distance_loss_list[i] + continuity_lambda * continuity_loss_list[i] + sparsity_lambda * sparsity_loss_list[i]\n",
    "        torch.save(cfdet.generator.state_dict(), './state_dict_minloss.bin')\n",
    "    if i == epochs2-1:\n",
    "        torch.save(cfdet.generator.state_dict(), './state_dict_final.bin')\n",
    "\n",
    "    print(f'epoch{i+1}:')\n",
    "    print('distance_loss:', distance_loss_list[i], 'continuity loss: ', continuity_loss_list[i], 'sparsity loss: ', sparsity_loss_list[i])\n",
    "    print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "QJ_yCy1-kWBM"
   },
   "outputs": [],
   "source": [
    "cfdet.generator.load_state_dict(torch.load('state_dict_minloss.bin')) \n",
    "\n",
    "y_key_pred3 = []\n",
    "y_key_truth3 = []\n",
    "\n",
    "cfdet.generator.eval()\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence, sequence_label, key_label, _ in val_loader:            \n",
    "        key_label_list = torch.reshape(key_label, (-1,)).tolist()\n",
    "        y_key_truth3 = y_key_truth3 + key_label_list\n",
    "\n",
    "        sequence = sequence.cuda()\n",
    "        z_out, _ = cfdet.generate(sequence, training=False)\n",
    "        z_list = torch.reshape(z_out, (-1,)).tolist()\n",
    "\n",
    "        y_key_pred3 = y_key_pred3 + z_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "z-PQ2h_ikWBM",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9956    0.9996    0.9976   1374486\n",
      "           1     0.9288    0.5547    0.6946     13514\n",
      "\n",
      "    accuracy                         0.9952   1388000\n",
      "   macro avg     0.9622    0.7771    0.8461   1388000\n",
      "weighted avg     0.9950    0.9952    0.9947   1388000\n",
      "\n",
      "[[1373911     575]\n",
      " [   6018    7496]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_key_truth3, y_key_pred3, digits=4))\n",
    "print(metrics.confusion_matrix(y_key_truth3, y_key_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "IKXKC0VIkWBM"
   },
   "outputs": [],
   "source": [
    "cfdet.generator.load_state_dict(torch.load('state_dict_minloss.bin')) \n",
    "\n",
    "seq_list_all = []\n",
    "\n",
    "y_key_pred = []\n",
    "y_key_truth = []\n",
    "\n",
    "cfdet.generator.eval()\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence, sequence_label, key_label, _ in train_test_loader2:  \n",
    "        seq_list_all += torch.reshape(sequence, (-1,)).tolist()\n",
    "        key_label_list = torch.reshape(key_label, (-1,)).tolist()\n",
    "        y_key_truth = y_key_truth + key_label_list\n",
    "\n",
    "        sequence = sequence.cuda()\n",
    "        z_out, _ = cfdet.generate(sequence, training=False)\n",
    "        z_list = torch.reshape(z_out, (-1,)).tolist()\n",
    "\n",
    "        y_key_pred = y_key_pred + z_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbHes52UkWBN",
    "outputId": "63b1f2b4-eacb-4471-a266-5c65640f8383",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9891    0.9916    0.9903    596521\n",
      "           1     0.9309    0.9119    0.9213     74219\n",
      "\n",
      "    accuracy                         0.9828    670740\n",
      "   macro avg     0.9600    0.9517    0.9558    670740\n",
      "weighted avg     0.9826    0.9828    0.9827    670740\n",
      "\n",
      "[[591494   5027]\n",
      " [  6539  67680]]\n",
      "0.9517343393441859\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_key_truth, y_key_pred, digits=4))\n",
    "print(metrics.confusion_matrix(y_key_truth, y_key_pred))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_key_truth, y_key_pred, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))\n",
    "\n",
    "f = open('output.txt', 'a')\n",
    "f.write('Entry anomaly detection on detected sequences:'+'\\n')\n",
    "f.write(str(metrics.classification_report(y_key_truth, y_key_pred, digits=4))+'\\n')\n",
    "f.write(str(metrics.confusion_matrix(y_key_truth, y_key_pred))+'\\n')\n",
    "f.write(str(metrics.auc(fpr, tpr))+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_key_pred2 = []\n",
    "y_key_truth2 = []\n",
    "\n",
    "\n",
    "for sequence, sequence_label, key_label, _ in train_test_loader3:  \n",
    "    seq_list_all += torch.reshape(sequence, (-1,)).tolist()\n",
    "    key_label_list = torch.reshape(key_label, (-1,)).tolist()\n",
    "    y_key_truth2 = y_key_truth2 + key_label_list   \n",
    "    y_key_pred2 = y_key_pred2 + [0]*len(key_label_list)\n",
    "        \n",
    "y_key_truth_all = y_key_truth + y_key_truth2\n",
    "y_key_pred_all = y_key_pred + y_key_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9956    0.9996    0.9976  12370109\n",
      "           1     0.9309    0.5559    0.6961    121751\n",
      "\n",
      "    accuracy                         0.9953  12491860\n",
      "   macro avg     0.9633    0.7777    0.8469  12491860\n",
      "weighted avg     0.9950    0.9953    0.9947  12491860\n",
      "\n",
      "[[12365082     5027]\n",
      " [   54071    67680]]\n",
      "0.7777411375804234\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_key_truth_all, y_key_pred_all, digits=4))\n",
    "print(metrics.confusion_matrix(y_key_truth_all, y_key_pred_all))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_key_truth_all, y_key_pred_all, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))\n",
    "\n",
    "f = open('output.txt', 'a')\n",
    "f.write('Entry anomaly detection on unlabeled dataset:'+'\\n')\n",
    "f.write(str(metrics.classification_report(y_key_truth_all, y_key_pred_all, digits=4))+'\\n')\n",
    "f.write(str(metrics.confusion_matrix(y_key_truth_all, y_key_pred_all))+'\\n')\n",
    "f.write(str(metrics.auc(fpr, tpr))+'\\n')\n",
    "f.write('-'*50+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rate(seq_list, truth_list, pred_list, idx):\n",
    "    counter1 = 0\n",
    "    counter2 = 0\n",
    "\n",
    "    for i in range(len(seq_list)):\n",
    "        if seq_list[i] == idx and truth_list[i]==1: \n",
    "            counter1 += 1\n",
    "            if pred_list[i]==1:\n",
    "                counter2 += 1            \n",
    "    return counter2, counter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logon  |detected:  0  |total:  0 \n",
      "\n",
      "Logoff  |detected:  0  |total:  0 \n",
      "\n",
      "Connect  |detected:  289  |total:  23510 \n",
      "\n",
      "Disconnect  |detected:  86  |total:  23395 \n",
      "\n",
      "Email  |detected:  0  |total:  7541 \n",
      "\n",
      "Http_normal  |detected:  0  |total:  0 \n",
      "\n",
      "Http_abnormal  |detected:  67305  |total:  67305 \n",
      "\n",
      "File  |detected:  0  |total:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_dict = {index:dict for dict,index in dict_activity.items()}\n",
    "\n",
    "for i in range(2,10,1):\n",
    "    counter, total = count_rate(seq_list_all, y_key_truth_all, y_key_pred_all, i)\n",
    "    print(activity_dict[i], ' |detected: ', counter, ' |total: ', total, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index2url = {index:url for url,index in url2index.items()}\n",
    "# activity_dict = {index:dict for dict,index in dict_activity.items()}\n",
    "\n",
    "# cfdet.generator.load_state_dict(torch.load('state_dict_minloss.bin')) \n",
    "\n",
    "# y_key_pred = []\n",
    "# y_key_truth = []\n",
    "\n",
    "# cfdet.generator.eval()\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for sequence, sequence_label, key_label, url in train_test_loader2:  \n",
    "#         f=open('Case.txt','a')\n",
    "#         key_label_list = key_label.tolist()\n",
    "\n",
    "#         for j in range(len(sequence_label)):\n",
    "#             y_key_truth = y_key_truth + key_label_list[j]\n",
    "\n",
    "#         sequence = sequence.cuda()\n",
    "#         z_out, _ = cfdet.generate(sequence, training=False)\n",
    "#         z_list = z_out.data.tolist()\n",
    "\n",
    "#         for k in range(len(sequence_label)):\n",
    "#             y_key_pred = y_key_pred + z_list[k]\n",
    "        \n",
    "#         for v in range(len(sequence_label)):\n",
    "# #             print(sequence.tolist()[v])\n",
    "# #             print(url.tolist()[v])                \n",
    "#             entry_list = [activity_dict.get(logkey) for logkey in sequence.tolist()[v]]\n",
    "#             url_list = [index2url.get(each) for each in url.tolist()[v]]\n",
    "# #             print([activity_dict.get(logkey) for logkey in sequence.tolist()[v]])\n",
    "# #             print([index2url.get(each) for each in url.tolist()[v]])\n",
    "#             for t in range(len(entry_list)):\n",
    "#                 if entry_list[t][:4] =='Http':\n",
    "#                     entry_list[t] = url_list[t][1:-1]\n",
    "# #             print(entry_list)\n",
    "#             f.write(str(entry_list)+'\\n')\n",
    "                    \n",
    "# #             print(key_label.tolist()[v])\n",
    "#             f.write('True label: ' + str(key_label.tolist()[v])+'\\n')\n",
    "# #             print(z_list[v])\n",
    "#             f.write('Pred label: ' + str(z_list[v])+'\\n')\n",
    "#             f.write('-----------------------'+'\\n')\n",
    "#         f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2718\n",
      "(572560, 4)\n",
      "(52033, 4)\n",
      "121751\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "\n",
    "print(test_data[test_data['Sequence_label']==0].shape)\n",
    "print(test_data[test_data['Sequence_label']==1].shape)\n",
    "\n",
    "test_data['Num'] = test_data['Key_label'].apply(lambda x: x.tolist().count(1))\n",
    "print(test_data['Num'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69400\n",
      "5782\n",
      "63618\n"
     ]
    }
   ],
   "source": [
    "print(val_data.shape[0])\n",
    "print(val_data[val_data['Sequence_label']==1].shape[0])\n",
    "print(val_data[val_data['Sequence_label']==0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13514"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data['Num'] = val_data['Key_label'].apply(lambda x: x.tolist().count(1))\n",
    "val_data['Num'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33537\n",
      "33537\n"
     ]
    }
   ],
   "source": [
    "print(train_test_data.shape[0])\n",
    "print(train_test_data2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1391559\n",
      "693993\n"
     ]
    }
   ],
   "source": [
    "print(raw_ds_normal.shape[0])\n",
    "print(raw_ds_abnormal.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Rationale_Thunderbird.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
