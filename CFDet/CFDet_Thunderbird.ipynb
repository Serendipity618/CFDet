{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4qMTHDxfeA1K"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from sklearn import manifold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DC-DBe_qeMhi"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kjuia1oReNza"
   },
   "outputs": [],
   "source": [
    "seed = 9\n",
    "\n",
    "def setup_seed(seed=seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "     torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('output.txt', 'a')\n",
    "f.write('seed: ' + str(seed) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eAB4y1QIePRh"
   },
   "outputs": [],
   "source": [
    "logdata = pd.read_csv(r'~/Python_projects/Rationale/Dataset/Thunderbird.log_structured.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Rk3yk9yfeQ1Z"
   },
   "outputs": [],
   "source": [
    "def slide_window(logdata, window_size = 20, step_size = 10):\n",
    "    logdata[\"Label\"] = logdata[\"Label\"].apply(lambda x: int(x != '-'))\n",
    "    data = logdata.loc[:, ['EventId', 'Label']]\n",
    "    data['Key_label'] = data['Label']\n",
    "    data.rename(columns={'Label':'Sequence_label'})\n",
    "    logkey = data['EventId']\n",
    "    logkey_label = data['Key_label']\n",
    "\n",
    "    new_data = []\n",
    "    idx = 0\n",
    "\n",
    "    while idx <= data.shape[0] - window_size:\n",
    "        new_data.append([\n",
    "                         logkey[idx : idx+window_size].values,\n",
    "                         max(logkey_label[idx : idx+window_size]),\n",
    "                         logkey_label[idx : idx+window_size].values\n",
    "                        ])\n",
    "        idx += step_size\n",
    "    return pd.DataFrame(new_data, columns = ['EventId', 'Sequence_label', 'Key_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "9jx15xtqeSZK",
    "outputId": "76e954a1-c044-4e2c-b3b7-93f3ac0e7f5b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>Sequence_label</th>\n",
       "      <th>Key_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[c89a99ae, 25be66dc, 25be66dc, 25be66dc, 25be6...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[25be66dc, 25be66dc, 25be66dc, 25be66dc, 25be6...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[25be66dc, 25be66dc, 25be66dc, 25be66dc, 25be6...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25be66dc, 25be66dc, 25be66dc, 25be66dc, 25be6...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[c89a99ae, 25be66dc, 25be66dc, 25be66dc, 25be6...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499994</th>\n",
       "      <td>[3b303a9d, 5f6a681c, f29fb486, f29fb486, 17469...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>[6391d957, b653d5bb, b653d5bb, a0088730, a0088...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>[f29fb486, 1746982b, 1746982b, 1746982b, a8ec9...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>[a0088730, a0088730, 4406c38d, 4406c38d, eb0cf...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>[df288d5d, f4455acc, f4455acc, a82970ae, f4455...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499999 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  EventId  Sequence_label  \\\n",
       "0       [c89a99ae, 25be66dc, 25be66dc, 25be66dc, 25be6...               0   \n",
       "1       [25be66dc, 25be66dc, 25be66dc, 25be66dc, 25be6...               0   \n",
       "2       [25be66dc, 25be66dc, 25be66dc, 25be66dc, 25be6...               0   \n",
       "3       [25be66dc, 25be66dc, 25be66dc, 25be66dc, 25be6...               0   \n",
       "4       [c89a99ae, 25be66dc, 25be66dc, 25be66dc, 25be6...               0   \n",
       "...                                                   ...             ...   \n",
       "499994  [3b303a9d, 5f6a681c, f29fb486, f29fb486, 17469...               1   \n",
       "499995  [6391d957, b653d5bb, b653d5bb, a0088730, a0088...               1   \n",
       "499996  [f29fb486, 1746982b, 1746982b, 1746982b, a8ec9...               1   \n",
       "499997  [a0088730, a0088730, 4406c38d, 4406c38d, eb0cf...               0   \n",
       "499998  [df288d5d, f4455acc, f4455acc, a82970ae, f4455...               0   \n",
       "\n",
       "                                                Key_label  \n",
       "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                   ...  \n",
       "499994  [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "499995  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "499996  [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "499997  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "499998  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[499999 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = slide_window(logdata)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GLgqsB_EeT0V"
   },
   "outputs": [],
   "source": [
    "normal_ds = dataset[dataset['Sequence_label']==0]\n",
    "abnormal_ds = dataset[dataset['Sequence_label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sdZen-UwfajL"
   },
   "outputs": [],
   "source": [
    "setup_seed()\n",
    "\n",
    "train_ds, rest_ds = train_test_split(normal_ds, test_size=0.2, random_state=2021)\n",
    "test_normal_ds, val_normal_ds = train_test_split(rest_ds, test_size=0.1, random_state=2021)\n",
    "test_abnormal_ds, val_abnormal_ds = train_test_split(abnormal_ds, test_size=0.1, random_state=2021)\n",
    "\n",
    "test_ds = pd.concat([test_normal_ds, test_abnormal_ds])\n",
    "val_ds = pd.concat([val_normal_ds, val_abnormal_ds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-CC-KhPDJ8f"
   },
   "source": [
    "**2. Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KyvIiONflmX8"
   },
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "\n",
    "for index, row in train_ds.iterrows():\n",
    "    counts.update(row['EventId'])\n",
    "\n",
    "logkey2index ={\"\":0,\"UNK\":1}\n",
    "logkeys = [\"\",\"UNK\"]\n",
    "\n",
    "for word in counts:\n",
    "    logkey2index[word] = len(logkeys)\n",
    "    logkeys.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whyFN9tPlvON",
    "outputId": "2447001c-5dc7-4f04-b404-fa1d404bd110",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hecheng/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1597: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "/home/hecheng/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "def encode_sequence(sequence, logkey2index):\n",
    "    return np.array([logkey2index.get(logkey, logkey2index[\"UNK\"]) for logkey in sequence])\n",
    "\n",
    "train_ds.loc[:,'Encoded'] = train_ds.loc[:,'EventId'].apply(lambda x: encode_sequence(x,logkey2index))\n",
    "test_ds.loc[:,'Encoded'] = test_ds.loc[:,'EventId'].apply(lambda x: encode_sequence(x,logkey2index))\n",
    "val_ds.loc[:,'Encoded'] = val_ds.loc[:,'EventId'].apply(lambda x: encode_sequence(x,logkey2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1ZfMwn-dm5U5"
   },
   "outputs": [],
   "source": [
    "train_data = train_ds[['Encoded', 'Sequence_label', 'Key_label']]\n",
    "test_data = test_ds[['Encoded', 'Sequence_label', 'Key_label']]\n",
    "val_data = val_ds[['Encoded', 'Sequence_label', 'Key_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ttqZYBggnVkU"
   },
   "outputs": [],
   "source": [
    "class LogDataset(Dataset):\n",
    "    def __init__(self, sequence, sequence_label, key_label):\n",
    "        self.sequence = sequence\n",
    "        self.sequence_label = sequence_label\n",
    "        self.key_label = key_label\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return (self.sequence[idx], self.sequence_label[idx], self.key_label[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 512\n",
    "batch_size_test = 4096\n",
    "batch_size_val = 4096\n",
    "batch_size_train_test = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "fLhwhscqnjOa"
   },
   "outputs": [],
   "source": [
    "setup_seed()\n",
    "\n",
    "def dataset_dataloader(data, batch_size):\n",
    "    sequence = data['Encoded'].tolist()\n",
    "    sequence_label = data['Sequence_label'].tolist()\n",
    "    key_label = data['Key_label'].tolist()\n",
    "    dataset = LogDataset(sequence, sequence_label, key_label)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    return data_loader\n",
    "\n",
    "train_loader = dataset_dataloader(train_data, batch_size = batch_size_train)\n",
    "test_loader = dataset_dataloader(test_data, batch_size = batch_size_test)\n",
    "val_loader = dataset_dataloader(val_data, batch_size = batch_size_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lf09uo30Z17N"
   },
   "source": [
    "**3. Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_mHXjMBWovar"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(logkeys)\n",
    "embedding_dim = 500\n",
    "hidden_dim = 512\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "W64xqXJKoyJe"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim=8, hidden_dim=64, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.randn(self.num_layers, x.size(0), self.hidden_dim).cuda()\n",
    "        c0 = torch.randn(self.num_layers, x.size(0), self.hidden_dim).cuda()\n",
    "\n",
    "        embedded = self.embeddings(x)\n",
    "        out, (hidden, cell) = self.lstm(embedded, (h0, c0))    \n",
    "        return torch.squeeze(torch.mean(out, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3Q2cZYJOozqi"
   },
   "outputs": [],
   "source": [
    "model = Net(vocab_size, embedding_dim, hidden_dim, num_layers).cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKpZa23Ha0hK",
    "outputId": "2e1b0ad9-2db5-4357-e73b-1b0bd6a6d4b5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  MSE:  0.00022579709965490607\n",
      "Epoch  2  MSE:  1.9128452471774638e-05\n",
      "Epoch  3  MSE:  8.251189798231823e-06\n",
      "Epoch  4  MSE:  4.4110847506143275e-06\n",
      "Epoch  5  MSE:  2.7049249841631328e-06\n",
      "Epoch  6  MSE:  1.8323982148714033e-06\n",
      "Epoch  7  MSE:  1.2799973015630654e-06\n",
      "Epoch  8  MSE:  9.207327077210538e-07\n",
      "Epoch  9  MSE:  6.935445477378134e-07\n",
      "Epoch  10  MSE:  5.35468103531221e-07\n",
      "Epoch  11  MSE:  4.3639740771029677e-07\n",
      "Epoch  12  MSE:  3.595440744090778e-07\n",
      "Epoch  13  MSE:  2.982253229887234e-07\n",
      "Epoch  14  MSE:  2.5397876437020996e-07\n",
      "Epoch  15  MSE:  2.200714889428315e-07\n",
      "Epoch  16  MSE:  1.897322822955594e-07\n",
      "Epoch  17  MSE:  1.6878964175728775e-07\n",
      "Epoch  18  MSE:  1.5240757368063807e-07\n",
      "Epoch  19  MSE:  1.3490660629399768e-07\n",
      "Epoch  20  MSE:  1.244976421187436e-07\n",
      "Epoch  21  MSE:  1.1243565483894531e-07\n",
      "Epoch  22  MSE:  1.0342523617384544e-07\n",
      "Epoch  23  MSE:  9.569590432936494e-08\n",
      "Epoch  24  MSE:  8.852098505436371e-08\n",
      "Epoch  25  MSE:  7.904105001792022e-08\n",
      "Epoch  26  MSE:  7.831589200912199e-08\n",
      "Epoch  27  MSE:  7.118414878552035e-08\n",
      "Epoch  28  MSE:  6.882826226573092e-08\n",
      "Epoch  29  MSE:  6.466181199582674e-08\n",
      "Epoch  30  MSE:  6.281648768646278e-08\n",
      "Epoch  31  MSE:  5.885559525422043e-08\n",
      "Epoch  32  MSE:  5.637009864699082e-08\n",
      "Epoch  33  MSE:  5.195938059010205e-08\n",
      "Epoch  34  MSE:  5.4137351862457424e-08\n",
      "Epoch  35  MSE:  4.797393544455419e-08\n",
      "Epoch  36  MSE:  4.824322922540995e-08\n",
      "Epoch  37  MSE:  4.8086673662259225e-08\n",
      "Epoch  38  MSE:  4.162253130335521e-08\n",
      "Epoch  39  MSE:  4.38025122815391e-08\n",
      "Epoch  40  MSE:  3.959814587948747e-08\n",
      "Epoch  41  MSE:  4.497899170486611e-08\n",
      "Epoch  42  MSE:  3.71185551959868e-08\n",
      "Epoch  43  MSE:  3.858654791747831e-08\n",
      "Epoch  44  MSE:  3.801926883332026e-08\n",
      "Epoch  45  MSE:  3.249504797356981e-08\n",
      "Epoch  46  MSE:  3.516063888386939e-08\n",
      "Epoch  47  MSE:  3.4040760783399163e-08\n",
      "Epoch  48  MSE:  4.466341300783029e-08\n",
      "Epoch  49  MSE:  2.985036238015469e-08\n",
      "Epoch  50  MSE:  3.058177386647747e-08\n"
     ]
    }
   ],
   "source": [
    "# if not os.path.exists('DeepSVDD.bin'):\n",
    "setup_seed()\n",
    "\n",
    "epochs = 50\n",
    "total_loss = []\n",
    "r_candidate = []\n",
    "min_loss = 10e6\n",
    "\n",
    "for i in range(epochs):\n",
    "    epoch_loss=[]\n",
    "    hidden_sum = torch.zeros((batch_size_train, hidden_dim))\n",
    "\n",
    "    if i < 20:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sequence, sequence_label, _ in train_loader:\n",
    "                sequence = sequence.cuda()\n",
    "                hidden_sum = hidden_sum.cuda()\n",
    "                hidden1 = model(sequence)\n",
    "                hidden_sum = hidden_sum + hidden1\n",
    "                sequence = sequence.cpu()\n",
    "\n",
    "\n",
    "        center = (torch.mean(hidden_sum.cuda(), axis=0) / len(train_loader))\n",
    "        center_batch = torch.repeat_interleave(torch.unsqueeze(center, 0), batch_size_train, dim=0).detach()\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    for sequence2, sequence_label2, _ in train_loader:\n",
    "        sequence2 = sequence2.cuda()\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        hidden2 = model(sequence2)  \n",
    "        loss = criterion(hidden2, center_batch.cuda())  \n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "#             if i == epochs-1:\n",
    "#                 r_candidate.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Epoch \", i+1, \" MSE: \", np.mean(epoch_loss))\n",
    "    total_loss.append(np.mean(epoch_loss))\n",
    "    if total_loss[i] < min_loss:\n",
    "        torch.save(model.state_dict(), './DeepSVDD.bin')\n",
    "        min_loss = total_loss[i]\n",
    "        r = total_loss[i]\n",
    "\n",
    "        f = open('center_radius.txt', 'w+')\n",
    "        f.write(str(center.tolist()))\n",
    "        f.write('\\n')\n",
    "        f.write(str(r))\n",
    "        f.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "bTmgJ_rXcVo9"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('DeepSVDD.bin'))\n",
    "\n",
    "f = open('center_radius.txt','r')\n",
    "center_radius = f.readlines()\n",
    "f.close()\n",
    "\n",
    "center = torch.tensor(eval(center_radius[0])).cuda()\n",
    "r = eval(center_radius[1])\n",
    "\n",
    "y_pred = []\n",
    "y_truth = []\n",
    "distance_list = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence, sequence_label, _ in val_loader: \n",
    "        y_truth = y_truth + sequence_label.tolist()\n",
    "\n",
    "        sequence = sequence.cuda()\n",
    "        hidden = model(sequence)\n",
    "        distance = torch.mean(torch.square(hidden-center), dim=1)\n",
    "        distance_list.extend(distance.tolist())\n",
    "        y_pred_batch = [int(i>r) for i in distance]\n",
    "        y_pred = y_pred + y_pred_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GK2i7KyGpHbf",
    "outputId": "1c339d60-37dd-494c-a884-d9363f9e498f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.8366    0.9110      6565\n",
      "           1     0.9284    1.0000    0.9629     13915\n",
      "\n",
      "    accuracy                         0.9476     20480\n",
      "   macro avg     0.9642    0.9183    0.9369     20480\n",
      "weighted avg     0.9514    0.9476    0.9462     20480\n",
      "\n",
      "[[ 5492  1073]\n",
      " [    0 13915]]\n",
      "0.9182787509520183\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_truth, y_pred, digits=4))\n",
    "print(metrics.confusion_matrix(y_truth, y_pred))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_truth, y_pred, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))\n",
    "\n",
    "f = open('output.txt', 'a')\n",
    "f.write('Sequence anomaly detection: '+'\\n')\n",
    "f.write(str(metrics.classification_report(y_truth, y_pred, digits=4))+'\\n')\n",
    "f.write(str(metrics.confusion_matrix(y_truth, y_pred))+'\\n')\n",
    "f.write(str(metrics.auc(fpr, tpr))+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('DeepSVDD.bin'))\n",
    "\n",
    "f = open('center_radius.txt','r')\n",
    "center_radius = f.readlines()\n",
    "f.close()\n",
    "\n",
    "center = torch.tensor(eval(center_radius[0])).cuda()\n",
    "r = eval(center_radius[1])\n",
    "\n",
    "y_pred = []\n",
    "y_truth = []\n",
    "seq_list = []\n",
    "distance_list = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence, sequence_label, _ in train_loader: \n",
    "        y_truth = y_truth + sequence_label.tolist()\n",
    "        seq_list += sequence.tolist()\n",
    "        sequence = sequence.cuda()\n",
    "        hidden = model(sequence)\n",
    "        distance = torch.mean(torch.square(hidden-center), dim=1)\n",
    "        distance_list += distance.tolist()\n",
    "        y_pred_batch = [int(i>r) for i in distance]\n",
    "        y_pred = y_pred + y_pred_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 48,  48,  48, 124, 125, 188, 189,  48, 188, 189,  48,  48,  48,  38,\n",
       "        295, 295,  36, 247,  38,  38], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_sequence = torch.tensor(seq_list[np.argmin(distance_list)]).to(device)\n",
    "baseline_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GSbFooWvkWBK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sequence_list = []\n",
    "sequence_label_list = []\n",
    "key_label_list = []\n",
    "\n",
    "sequence_list2 = []\n",
    "sequence_label_list2 = []\n",
    "key_label_list2 = []\n",
    "\n",
    "sequence_list3 = []\n",
    "sequence_label_list3 = []\n",
    "key_label_list3 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence, sequence_label, key_label in test_loader: \n",
    "        sequence = sequence.cuda()\n",
    "        \n",
    "        hidden = model(sequence)\n",
    "        distance = torch.mean(torch.square(hidden-center), dim=1)\n",
    "        y_pred_index_batch = [i for i in range(len(distance)) if distance[i]>r]\n",
    "        y_pred_index_batch2 = [i for i in range(len(distance)) if distance[i]>r]\n",
    "        y_pred_index_batch3 = [i for i in range(len(distance)) if distance[i]<=r]\n",
    "        \n",
    "        sequence_l = sequence.tolist()\n",
    "        sequence_label_l = sequence_label.tolist()\n",
    "        key_label_l = key_label.tolist()\n",
    "        \n",
    "        for i in y_pred_index_batch:\n",
    "            sequence_list += [sequence_l[i]]\n",
    "            sequence_label_list += [sequence_label_l[i]]\n",
    "            key_label_list += [key_label_l[i]]\n",
    "            \n",
    "        for j in y_pred_index_batch2:\n",
    "            sequence_list2 += [sequence_l[j]]\n",
    "            sequence_label_list2 += [sequence_label_l[j]]\n",
    "            key_label_list2 += [key_label_l[j]]\n",
    "            \n",
    "        for k in y_pred_index_batch3:\n",
    "            sequence_list3 += [sequence_l[k]]\n",
    "            sequence_label_list3 += [sequence_label_l[k]]\n",
    "            key_label_list3 += [key_label_l[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "aalNggSKkWBK"
   },
   "outputs": [],
   "source": [
    "def train_test_data_loader(sequence_list, sequence_label_list, key_label_list):\n",
    "    d = {'Encoded': sequence_list,\n",
    "         'Sequence_label': sequence_label_list,\n",
    "         'Key_label': key_label_list}\n",
    "\n",
    "    train_test_data = pd.DataFrame(d)\n",
    "\n",
    "    train_test_data['Encoded'] = [torch.tensor(i) for i in train_test_data['Encoded']]\n",
    "    train_test_data['Sequence_label'] = [torch.tensor(i) for i in train_test_data['Sequence_label']]\n",
    "    train_test_data['Key_label'] = [torch.tensor(i) for i in train_test_data['Key_label']]\n",
    "\n",
    "    train_test_loader = dataset_dataloader(train_test_data, batch_size = batch_size_train_test)\n",
    "    return train_test_loader, train_test_data\n",
    "\n",
    "train_test_loader, train_test_data   = train_test_data_loader(sequence_list, sequence_label_list, key_label_list)\n",
    "train_test_loader2, train_test_data2 = train_test_data_loader(sequence_list2, sequence_label_list2, key_label_list2)\n",
    "train_test_loader3, train_test_data3 = train_test_data_loader(sequence_list3, sequence_label_list3, key_label_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "id": "wWxEAjsykWBK"
   },
   "outputs": [],
   "source": [
    "embedding_dim2 = 100\n",
    "hidden_dim2 = 128\n",
    "num_layers2 = 1\n",
    "triplet_lambda = 1\n",
    "continuity_lambda = 0.1\n",
    "sparsity_lambda = 0.1\n",
    "epochs2 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=256, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embeddings(x)\n",
    "        out, (hidden, cell) = self.lstm(embedded)    \n",
    "        scores = self.output_layer(out) \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "id": "D93Iozz-kWBL"
   },
   "outputs": [],
   "source": [
    "class CFDet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CFDet, self).__init__()\n",
    "        self.exploration_rate = 0.05\n",
    "        self.count_tokens = 3\n",
    "        self.count_pieces = 3\n",
    "        self.generator = Generator(vocab_size, embedding_dim2, hidden_dim2, num_layers2).cuda()\n",
    "\n",
    "    def generate(self, x, training=True):\n",
    "        z_scores_ = self.generator(x)\n",
    "        z_probs_ = F.softmax(z_scores_, dim=-1)\n",
    "        z_prob_ = (1 - self.exploration_rate) * z_probs_ + self.exploration_rate / z_probs_.size(-1)\n",
    "        z_prob__ = z_prob_.view(-1, 2)\n",
    "        sampler = torch.distributions.Categorical(z_prob__)\n",
    "\n",
    "        if training:\n",
    "            z_ = sampler.sample()  # (num_rows * p_length,)\n",
    "            z = z_.view(z_prob_.size(0), z_prob_.size(1))\n",
    "            z = z.type(torch.cuda.IntTensor)\n",
    "            neg_log_probs_ = -sampler.log_prob(z_)\n",
    "            neg_log_probs = neg_log_probs_.view(z_prob_.size(0), z_prob_.size(1))\n",
    "            return z, neg_log_probs\n",
    "        else:\n",
    "            z__index = torch.max(z_prob__, dim=-1)[1]\n",
    "            z0 = z__index.view(z_prob_.size(0), z_prob_.size(1))\n",
    "            z_index = z0.type(torch.cuda.IntTensor)\n",
    "\n",
    "            z__value = torch.max(z_prob__, dim=-1)[0]\n",
    "            # z1 = z__value.view(z_prob_.size(0), z_prob_.size(1))\n",
    "            z_value = z__value.type(torch.cuda.FloatTensor)\n",
    "            return z_index, z_value\n",
    "\n",
    "    def get_loss(self, x, z, neg_log_probs,average_reward, batch_size, model, sequence_length=20.0):\n",
    "        z_ = torch.cat([z[:, 1:], z[:, -1:]], dim=-1)\n",
    "        continuity_ratio = torch.div(torch.sum(torch.abs(z - z_), dim=-1), sequence_length)\n",
    "        percentage = (self.count_pieces-1) / sequence_length\n",
    "        continuity_loss = torch.abs(continuity_ratio - percentage)\n",
    "#         continuity_loss = torch.clamp(continuity_ratio - percentage, min=0)\n",
    "\n",
    "\n",
    "        sparsity_ratio = torch.div(torch.sum(z, dim=-1), sequence_length)\n",
    "        percentage = self.count_tokens / sequence_length\n",
    "        sparsity_loss = torch.abs(sparsity_ratio - percentage)\n",
    "#         sparsity_loss = torch.clamp(sparsity_ratio - percentage, min=0)\n",
    "\n",
    "        anomalous_entry = x * z + baseline_sequence * (1-z)\n",
    "        anti = x * (1-z) + baseline_sequence * z\n",
    "        hidden_anomalous_entry = model(anomalous_entry)\n",
    "        hidden_anti = model(anti)\n",
    "        distance_loss = criterion2(center_batch2, hidden_anti, hidden_anomalous_entry) + criterion(center_batch2, hidden_anti) \\\n",
    "                        - criterion(center_batch2, hidden_anomalous_entry)      \n",
    "        average_reward = average_reward.cuda()\n",
    "        rewards = -(triplet_lambda * distance_loss + sparsity_lambda * sparsity_loss + continuity_lambda * continuity_loss ).detach()\n",
    "        advantages = rewards - average_reward # (batch_size,)\n",
    "\n",
    "        advantages_expand_ = advantages.unsqueeze(-1).expand_as(neg_log_probs)       \n",
    "        rl_loss = torch.sum(neg_log_probs * advantages_expand_)\n",
    "        \n",
    "        return distance_loss, rl_loss, rewards, continuity_loss, sparsity_loss, advantages_expand_\n",
    "\n",
    "    def training_step(self, distance_loss, rl_loss):\n",
    "        rl_loss.backward()\n",
    "        optimiser2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "id": "Hj_YWgagkWBL"
   },
   "outputs": [],
   "source": [
    "cfdet = CFDet()\n",
    "criterion2 = nn.TripletMarginLoss(margin=1, reduction='none')\n",
    "optimiser2 = optim.Adam(cfdet.generator.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z859sJLQkWBM",
    "outputId": "46554397-351f-4c88-830c-b5e5cfba80c5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1:\n",
      "distance_loss: 0.936820166025843 continuity loss:  0.14446010515093805 sparsity loss:  0.2369712607668979\n",
      "------------------------------------------------------\n",
      "epoch2:\n",
      "distance_loss: 0.930449441075325 continuity loss:  0.08292585435722555 sparsity loss:  0.11684709947024073\n",
      "------------------------------------------------------\n",
      "epoch3:\n",
      "distance_loss: 0.9295507460832596 continuity loss:  0.08139892965555191 sparsity loss:  0.12215227436806475\n",
      "------------------------------------------------------\n",
      "epoch4:\n",
      "distance_loss: 0.9296671471425465 continuity loss:  0.08058001159557275 sparsity loss:  0.12122209870389529\n",
      "------------------------------------------------------\n",
      "epoch5:\n",
      "distance_loss: 0.9294748531920569 continuity loss:  0.08102783566074712 sparsity loss:  0.1202242617628404\n",
      "------------------------------------------------------\n",
      "epoch6:\n",
      "distance_loss: 0.9295838905232293 continuity loss:  0.08049491138330528 sparsity loss:  0.12087995393999985\n",
      "------------------------------------------------------\n",
      "epoch7:\n",
      "distance_loss: 0.9294745917831149 continuity loss:  0.08096331256840911 sparsity loss:  0.12157924138009549\n",
      "------------------------------------------------------\n",
      "epoch8:\n",
      "distance_loss: 0.9293168412787574 continuity loss:  0.08094971062881606 sparsity loss:  0.12226109206676483\n",
      "------------------------------------------------------\n",
      "epoch9:\n",
      "distance_loss: 0.9292716792651585 continuity loss:  0.08071254529058933 sparsity loss:  0.12127964634980475\n",
      "------------------------------------------------------\n",
      "epoch10:\n",
      "distance_loss: 0.9292285046407155 continuity loss:  0.08074323747839246 sparsity loss:  0.12130475826561452\n",
      "------------------------------------------------------\n",
      "epoch11:\n",
      "distance_loss: 0.9292647761957986 continuity loss:  0.08035714626312256 sparsity loss:  0.1207285856029817\n",
      "------------------------------------------------------\n",
      "epoch12:\n",
      "distance_loss: 0.9296081700495311 continuity loss:  0.08020647657769067 sparsity loss:  0.12610386461019515\n",
      "------------------------------------------------------\n",
      "epoch13:\n",
      "distance_loss: 0.9288536531584604 continuity loss:  0.0803292443709714 sparsity loss:  0.12456821948289871\n",
      "------------------------------------------------------\n",
      "epoch14:\n",
      "distance_loss: 0.9305802766765867 continuity loss:  0.07932756995516164 sparsity loss:  0.10857631234186037\n",
      "------------------------------------------------------\n",
      "epoch15:\n",
      "distance_loss: 0.9347207976239068 continuity loss:  0.0777162422559091 sparsity loss:  0.08159737837101733\n",
      "------------------------------------------------------\n",
      "epoch16:\n",
      "distance_loss: 0.9330652475357055 continuity loss:  0.07887277129505361 sparsity loss:  0.09425816312432289\n",
      "------------------------------------------------------\n",
      "epoch17:\n",
      "distance_loss: 0.9334725601332529 continuity loss:  0.07860770422433104 sparsity loss:  0.09334298398877894\n",
      "------------------------------------------------------\n",
      "epoch18:\n",
      "distance_loss: 0.9305721759796143 continuity loss:  0.07959821767040662 sparsity loss:  0.11044398775058133\n",
      "------------------------------------------------------\n",
      "epoch19:\n",
      "distance_loss: 0.9288610760654722 continuity loss:  0.08024588791387421 sparsity loss:  0.12421317026019096\n",
      "------------------------------------------------------\n",
      "epoch20:\n",
      "distance_loss: 0.9288124991314751 continuity loss:  0.0811150284750121 sparsity loss:  0.12270438186824321\n",
      "------------------------------------------------------\n",
      "epoch21:\n",
      "distance_loss: 0.9286378387893949 continuity loss:  0.08076032772660255 sparsity loss:  0.12869698745863778\n",
      "------------------------------------------------------\n",
      "epoch22:\n",
      "distance_loss: 0.9285610807793481 continuity loss:  0.08114990554749965 sparsity loss:  0.13056919729071004\n",
      "------------------------------------------------------\n",
      "epoch23:\n",
      "distance_loss: 0.928863354240145 continuity loss:  0.07999791066561426 sparsity loss:  0.1215098362416029\n",
      "------------------------------------------------------\n",
      "epoch24:\n",
      "distance_loss: 0.9286567679473332 continuity loss:  0.08037039946232523 sparsity loss:  0.1222917831901993\n",
      "------------------------------------------------------\n",
      "epoch25:\n",
      "distance_loss: 0.9290562710591725 continuity loss:  0.08014962662543569 sparsity loss:  0.12141566771481718\n",
      "------------------------------------------------------\n",
      "epoch26:\n",
      "distance_loss: 0.9287368753126689 continuity loss:  0.08028843802000796 sparsity loss:  0.12379534116813115\n",
      "------------------------------------------------------\n",
      "epoch27:\n",
      "distance_loss: 0.9286343953439168 continuity loss:  0.07989293086741651 sparsity loss:  0.12347516843250819\n",
      "------------------------------------------------------\n",
      "epoch28:\n",
      "distance_loss: 0.9318209337336677 continuity loss:  0.07888323441147804 sparsity loss:  0.09713832390095506\n",
      "------------------------------------------------------\n",
      "epoch29:\n",
      "distance_loss: 0.9298328446490424 continuity loss:  0.07983852167214667 sparsity loss:  0.11172119171491691\n",
      "------------------------------------------------------\n",
      "epoch30:\n",
      "distance_loss: 0.9341660699674061 continuity loss:  0.07787493360894067 sparsity loss:  0.08176583504038197\n",
      "------------------------------------------------------\n",
      "epoch31:\n",
      "distance_loss: 0.9318160048552921 continuity loss:  0.07898298331669398 sparsity loss:  0.09328648203185626\n",
      "------------------------------------------------------\n",
      "epoch32:\n",
      "distance_loss: 0.9307061727557864 continuity loss:  0.07962332958621639 sparsity loss:  0.1014791437025581\n",
      "------------------------------------------------------\n",
      "epoch33:\n",
      "distance_loss: 0.9295878997870854 continuity loss:  0.0799511755151408 sparsity loss:  0.11036795478846345\n",
      "------------------------------------------------------\n",
      "epoch34:\n",
      "distance_loss: 0.9296550303697586 continuity loss:  0.07965402140149049 sparsity loss:  0.11163748641099248\n",
      "------------------------------------------------------\n",
      "epoch35:\n",
      "distance_loss: 0.9290862900870187 continuity loss:  0.08002058102616243 sparsity loss:  0.11772356341992106\n",
      "------------------------------------------------------\n",
      "epoch36:\n",
      "distance_loss: 0.92872183237757 continuity loss:  0.080365168516125 sparsity loss:  0.12104736364313534\n",
      "------------------------------------------------------\n",
      "epoch37:\n",
      "distance_loss: 0.9288383317845208 continuity loss:  0.08020473314183099 sparsity loss:  0.11952776222356729\n",
      "------------------------------------------------------\n",
      "epoch38:\n",
      "distance_loss: 0.929612147808075 continuity loss:  0.08001569807529449 sparsity loss:  0.11058279877262456\n",
      "------------------------------------------------------\n",
      "epoch39:\n",
      "distance_loss: 0.9287461570331028 continuity loss:  0.0805287421814033 sparsity loss:  0.11973597912916116\n",
      "------------------------------------------------------\n",
      "epoch40:\n",
      "distance_loss: 0.9290104627609252 continuity loss:  0.08043980566518648 sparsity loss:  0.11918561703392437\n",
      "------------------------------------------------------\n",
      "epoch41:\n",
      "distance_loss: 0.9290032578366143 continuity loss:  0.08018276036850043 sparsity loss:  0.11898367793432303\n",
      "------------------------------------------------------\n",
      "epoch42:\n",
      "distance_loss: 0.9291311881371906 continuity loss:  0.08012451540146555 sparsity loss:  0.11872837719108377\n",
      "------------------------------------------------------\n",
      "epoch43:\n",
      "distance_loss: 0.9289964722735541 continuity loss:  0.08036481919033187 sparsity loss:  0.11879987525088447\n",
      "------------------------------------------------------\n",
      "epoch44:\n",
      "distance_loss: 0.9289942353963851 continuity loss:  0.08001534970743315 sparsity loss:  0.11964878743248326\n",
      "------------------------------------------------------\n",
      "epoch45:\n",
      "distance_loss: 0.9290218234062195 continuity loss:  0.08031006210616656 sparsity loss:  0.11964599740292345\n",
      "------------------------------------------------------\n",
      "epoch46:\n",
      "distance_loss: 0.9285751896245139 continuity loss:  0.07987967697637421 sparsity loss:  0.12066231966018677\n",
      "------------------------------------------------------\n",
      "epoch47:\n",
      "distance_loss: 0.9285480103322438 continuity loss:  0.0804450370903526 sparsity loss:  0.12161865351455552\n",
      "------------------------------------------------------\n",
      "epoch48:\n",
      "distance_loss: 0.9289680923734392 continuity loss:  0.08031250366142818 sparsity loss:  0.11923060917428562\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch49:\n",
      "distance_loss: 0.9290000936814717 continuity loss:  0.08021694033273628 sparsity loss:  0.11876953217600072\n",
      "------------------------------------------------------\n",
      "epoch50:\n",
      "distance_loss: 0.9292024476187569 continuity loss:  0.080011861877782 sparsity loss:  0.11866629501538617\n",
      "------------------------------------------------------\n",
      "epoch51:\n",
      "distance_loss: 0.9291536301374436 continuity loss:  0.07988630421459675 sparsity loss:  0.11839460161115442\n",
      "------------------------------------------------------\n",
      "epoch52:\n",
      "distance_loss: 0.9290625861712865 continuity loss:  0.07975725824279445 sparsity loss:  0.1189407793538911\n",
      "------------------------------------------------------\n",
      "epoch53:\n",
      "distance_loss: 0.9289943350212915 continuity loss:  0.07978027665189334 sparsity loss:  0.11901995060699326\n",
      "------------------------------------------------------\n",
      "epoch54:\n",
      "distance_loss: 0.928634986281395 continuity loss:  0.07984061438058104 sparsity loss:  0.12436000342879977\n",
      "------------------------------------------------------\n",
      "epoch55:\n",
      "distance_loss: 0.9285887701170785 continuity loss:  0.07978027713085924 sparsity loss:  0.12765904100877898\n",
      "------------------------------------------------------\n",
      "epoch56:\n",
      "distance_loss: 0.9323279397828238 continuity loss:  0.07873500622808934 sparsity loss:  0.08817243293992111\n",
      "------------------------------------------------------\n",
      "epoch57:\n",
      "distance_loss: 0.9308049184935433 continuity loss:  0.07906215467623302 sparsity loss:  0.09835518939154489\n",
      "------------------------------------------------------\n",
      "epoch58:\n",
      "distance_loss: 0.9294156168188368 continuity loss:  0.07982596579406942 sparsity loss:  0.11114746098007475\n",
      "------------------------------------------------------\n",
      "epoch59:\n",
      "distance_loss: 0.9287994201694216 continuity loss:  0.08023228565497058 sparsity loss:  0.11727504219327654\n",
      "------------------------------------------------------\n",
      "epoch60:\n",
      "distance_loss: 0.9291945402111326 continuity loss:  0.07985735509012426 sparsity loss:  0.11384242522929396\n",
      "------------------------------------------------------\n",
      "epoch61:\n",
      "distance_loss: 0.9285869917699269 continuity loss:  0.08017962143889495 sparsity loss:  0.12339983209967613\n",
      "------------------------------------------------------\n",
      "epoch62:\n",
      "distance_loss: 0.9284729991640364 continuity loss:  0.08057757041284017 sparsity loss:  0.12571568159120425\n",
      "------------------------------------------------------\n",
      "epoch63:\n",
      "distance_loss: 0.9286374126161848 continuity loss:  0.08016497285238335 sparsity loss:  0.12175083804343428\n",
      "------------------------------------------------------\n",
      "epoch64:\n",
      "distance_loss: 0.9286947795322963 continuity loss:  0.08020229137369565 sparsity loss:  0.12058070674538612\n",
      "------------------------------------------------------\n",
      "epoch65:\n",
      "distance_loss: 0.928546022943088 continuity loss:  0.0802008966250079 sparsity loss:  0.12213971859642438\n",
      "------------------------------------------------------\n",
      "epoch66:\n",
      "distance_loss: 0.9285742138113294 continuity loss:  0.08018729420644896 sparsity loss:  0.12150076750133719\n",
      "------------------------------------------------------\n",
      "epoch67:\n",
      "distance_loss: 0.928662086384637 continuity loss:  0.08045863871063505 sparsity loss:  0.1206302317657641\n",
      "------------------------------------------------------\n",
      "epoch68:\n",
      "distance_loss: 0.9284089846270425 continuity loss:  0.08081892100828035 sparsity loss:  0.12207275394882475\n",
      "------------------------------------------------------\n",
      "epoch69:\n",
      "distance_loss: 0.9289596859897886 continuity loss:  0.08054897151887416 sparsity loss:  0.11892613103347165\n",
      "------------------------------------------------------\n",
      "epoch70:\n",
      "distance_loss: 0.9289955743721553 continuity loss:  0.08040562628635338 sparsity loss:  0.1186042135315282\n",
      "------------------------------------------------------\n",
      "epoch71:\n",
      "distance_loss: 0.9300182389361518 continuity loss:  0.07934326561433928 sparsity loss:  0.10605364191745009\n",
      "------------------------------------------------------\n",
      "epoch72:\n",
      "distance_loss: 0.9302752507584435 continuity loss:  0.07927002316074712 sparsity loss:  0.10433593807475908\n",
      "------------------------------------------------------\n",
      "epoch73:\n",
      "distance_loss: 0.9297116841588702 continuity loss:  0.07920270998563085 sparsity loss:  0.10689906626939774\n",
      "------------------------------------------------------\n",
      "epoch74:\n",
      "distance_loss: 0.9298044464417866 continuity loss:  0.07932477997882026 sparsity loss:  0.1069276647908347\n",
      "------------------------------------------------------\n",
      "epoch75:\n",
      "distance_loss: 0.9288819372653961 continuity loss:  0.08023716807365418 sparsity loss:  0.11838937087782792\n",
      "------------------------------------------------------\n",
      "epoch76:\n",
      "distance_loss: 0.928871791277613 continuity loss:  0.0797551657472338 sparsity loss:  0.11702357839260782\n",
      "------------------------------------------------------\n",
      "epoch77:\n",
      "distance_loss: 0.9287042945623398 continuity loss:  0.08011056418929781 sparsity loss:  0.11980329241071429\n",
      "------------------------------------------------------\n",
      "epoch78:\n",
      "distance_loss: 0.9287501386233739 continuity loss:  0.07990409093243735 sparsity loss:  0.11906180285981723\n",
      "------------------------------------------------------\n",
      "epoch79:\n",
      "distance_loss: 0.9285472827298301 continuity loss:  0.08015416104878698 sparsity loss:  0.12022705115377903\n",
      "------------------------------------------------------\n",
      "epoch80:\n",
      "distance_loss: 0.928617754152843 continuity loss:  0.08032261835677283 sparsity loss:  0.12032121845654078\n",
      "------------------------------------------------------\n",
      "epoch81:\n",
      "distance_loss: 0.9291133237736565 continuity loss:  0.07941511246774878 sparsity loss:  0.11215401754847594\n",
      "------------------------------------------------------\n",
      "epoch82:\n",
      "distance_loss: 0.9288117421524865 continuity loss:  0.07986433197345053 sparsity loss:  0.11614536850580147\n",
      "------------------------------------------------------\n",
      "epoch83:\n",
      "distance_loss: 0.9304443091154099 continuity loss:  0.07931989740048136 sparsity loss:  0.09878278475786959\n",
      "------------------------------------------------------\n",
      "epoch84:\n",
      "distance_loss: 0.9296345489365714 continuity loss:  0.07915667252881187 sparsity loss:  0.10581647620669433\n",
      "------------------------------------------------------\n",
      "epoch85:\n",
      "distance_loss: 0.9303299959216799 continuity loss:  0.07846261475767408 sparsity loss:  0.09875662662088872\n",
      "------------------------------------------------------\n",
      "epoch86:\n",
      "distance_loss: 0.9321822558130537 continuity loss:  0.07825683892837593 sparsity loss:  0.08688616071428572\n",
      "------------------------------------------------------\n",
      "epoch87:\n",
      "distance_loss: 0.92972554734775 continuity loss:  0.07891706579497883 sparsity loss:  0.10626883304544858\n",
      "------------------------------------------------------\n",
      "epoch88:\n",
      "distance_loss: 0.9294992434126991 continuity loss:  0.07926130358661924 sparsity loss:  0.11006835933242526\n",
      "------------------------------------------------------\n",
      "epoch89:\n",
      "distance_loss: 0.9290392058236259 continuity loss:  0.07956055024904864 sparsity loss:  0.11316057491515363\n",
      "------------------------------------------------------\n",
      "epoch90:\n",
      "distance_loss: 0.9288269085543496 continuity loss:  0.07964007072150707 sparsity loss:  0.11726004465350083\n",
      "------------------------------------------------------\n",
      "epoch91:\n",
      "distance_loss: 0.9289003197635923 continuity loss:  0.08026890690837588 sparsity loss:  0.11815638914704323\n",
      "------------------------------------------------------\n",
      "epoch92:\n",
      "distance_loss: 0.929817971587181 continuity loss:  0.07910365845475878 sparsity loss:  0.10399797734405314\n",
      "------------------------------------------------------\n",
      "epoch93:\n",
      "distance_loss: 0.9291724298681532 continuity loss:  0.07944998938058104 sparsity loss:  0.11036969854363374\n",
      "------------------------------------------------------\n",
      "epoch94:\n",
      "distance_loss: 0.9295921087265014 continuity loss:  0.07900739753884929 sparsity loss:  0.10547084260199752\n",
      "------------------------------------------------------\n",
      "epoch95:\n",
      "distance_loss: 0.9301789360386985 continuity loss:  0.07923863322607108 sparsity loss:  0.10274902348007475\n",
      "------------------------------------------------------\n",
      "epoch96:\n",
      "distance_loss: 0.930296984740666 continuity loss:  0.07907087419714247 sparsity loss:  0.0999766323183264\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch97:\n",
      "distance_loss: 0.9302503956215722 continuity loss:  0.0790652940848044 sparsity loss:  0.09970040491649083\n",
      "------------------------------------------------------\n",
      "epoch98:\n",
      "distance_loss: 0.9307265903268541 continuity loss:  0.0784242500684091 sparsity loss:  0.09546770341694355\n",
      "------------------------------------------------------\n",
      "epoch99:\n",
      "distance_loss: 0.9306097154106413 continuity loss:  0.07865234687924386 sparsity loss:  0.09525495256696428\n",
      "------------------------------------------------------\n",
      "epoch100:\n",
      "distance_loss: 0.9293365742479052 continuity loss:  0.07934291660785675 sparsity loss:  0.10858851825552328\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# if not os.path.exists('state_dict_minloss.bin'):\n",
    "\n",
    "setup_seed()\n",
    "\n",
    "total_loss_list = []\n",
    "distance_loss_list = []\n",
    "reward_list = []\n",
    "continuity_loss_list = []\n",
    "sparsity_loss_list = []\n",
    "loss_list = []\n",
    "\n",
    "min_loss= 10e6\n",
    "\n",
    "center_batch2 = torch.repeat_interleave(torch.unsqueeze(center, 0), batch_size_train_test, dim=0).cuda()\n",
    "\n",
    "for i in range(epochs2):\n",
    "    z_history_rewards = deque(maxlen=200)\n",
    "    z_history_rewards.append(0.0)\n",
    "    epoch_distance_loss = []\n",
    "    epoch_continuity_loss = []\n",
    "    epoch_sparsity_loss = []\n",
    "    epoch_rl_loss = []\n",
    "    epoch_reward = []\n",
    "    epoch_loss= []\n",
    "\n",
    "    cfdet.generator.train()\n",
    "    model.train()\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False \n",
    "\n",
    "    for sequence4, sequence_label4, _ in train_test_loader:\n",
    "        sequence4 = sequence4.cuda()\n",
    "\n",
    "        baseline = Variable(torch.FloatTensor([float(np.mean(z_history_rewards))]))\n",
    "\n",
    "        if len(sequence_label4) == batch_size_train_test:             \n",
    "            optimiser2.zero_grad()\n",
    "\n",
    "            z, neg_log_probs = cfdet.generate(sequence4)\n",
    "            distance_loss, rl_loss, rewards, continuity_loss, sparsity_loss, advantage = cfdet.get_loss(sequence4, z, neg_log_probs, baseline, batch_size_train_test, model)\n",
    "            cfdet.training_step(distance_loss, rl_loss)\n",
    "\n",
    "            epoch_distance_loss.append(torch.mean(distance_loss).item())\n",
    "            epoch_continuity_loss.append(torch.mean(continuity_loss).item())\n",
    "            epoch_sparsity_loss.append(torch.mean(sparsity_loss).item())\n",
    "            epoch_rl_loss.append(rl_loss.item())\n",
    "            epoch_reward.append(torch.sum(rewards).item())\n",
    "            epoch_loss.append(torch.sum(-rewards).item())\n",
    "\n",
    "            z_batch_reward = np.mean(rewards.cpu().data.numpy())\n",
    "            z_history_rewards.append(z_batch_reward)\n",
    "\n",
    "    total_loss_list.append(np.mean(epoch_rl_loss))\n",
    "    continuity_loss_list.append(np.mean(epoch_continuity_loss))\n",
    "    sparsity_loss_list.append(np.mean(epoch_sparsity_loss))\n",
    "    distance_loss_list.append(np.mean(epoch_distance_loss))\n",
    "    reward_list.append(np.mean(epoch_reward))\n",
    "    loss_list.append(np.mean(epoch_loss))\n",
    "\n",
    "    if distance_loss_list[i] + continuity_lambda * continuity_loss_list[i] + sparsity_lambda * sparsity_loss_list[i] < min_loss:\n",
    "        min_loss = distance_loss_list[i] + continuity_lambda * continuity_loss_list[i] + sparsity_lambda * sparsity_loss_list[i]\n",
    "        torch.save(cfdet.generator.state_dict(), './state_dict_minloss.bin')\n",
    "    if i == epochs2-1:\n",
    "        torch.save(cfdet.generator.state_dict(), './state_dict_final.bin')\n",
    "\n",
    "    print(f'epoch{i+1}:')\n",
    "    print('distance_loss:', distance_loss_list[i], 'continuity loss: ', continuity_loss_list[i], 'sparsity loss: ', sparsity_loss_list[i])\n",
    "    print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "QJ_yCy1-kWBM"
   },
   "outputs": [],
   "source": [
    "cfdet.generator.load_state_dict(torch.load('state_dict_minloss.bin')) \n",
    "# cfdet.generator.load_state_dict(torch.load('state_dict_final.bin'))\n",
    "\n",
    "y_key_pred2 = []\n",
    "y_key_truth2 = []\n",
    "\n",
    "cfdet.generator.eval()\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence, sequence_label, key_label in val_loader: \n",
    "        key_label_list = key_label.tolist()\n",
    "\n",
    "        for j in range(len(sequence_label)):\n",
    "            y_key_truth2 = y_key_truth2 + key_label_list[j]\n",
    "\n",
    "        sequence = sequence.cuda()\n",
    "        z_out, _ = cfdet.generate(sequence, training=False)\n",
    "        z_list = z_out.data.tolist()\n",
    "\n",
    "        for k in range(len(sequence_label)):\n",
    "            y_key_pred2 = y_key_pred2 + z_list[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "id": "z-PQ2h_ikWBM",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9341    0.9659    367305\n",
      "           1     0.6360    1.0000    0.7775     42295\n",
      "\n",
      "    accuracy                         0.9409    409600\n",
      "   macro avg     0.8180    0.9670    0.8717    409600\n",
      "weighted avg     0.9624    0.9409    0.9465    409600\n",
      "\n",
      "[[343100  24205]\n",
      " [     1  42294]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_key_truth2, y_key_pred2, digits=4))\n",
    "print(metrics.confusion_matrix(y_key_truth2, y_key_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "id": "IKXKC0VIkWBM"
   },
   "outputs": [],
   "source": [
    "cfdet.generator.load_state_dict(torch.load('state_dict_minloss.bin')) \n",
    "\n",
    "y_key_pred = []\n",
    "y_key_truth = []\n",
    "\n",
    "cfdet.generator.eval()\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence, sequence_label, key_label in train_test_loader2:            \n",
    "        key_label_list = torch.reshape(key_label, (-1,)).tolist()\n",
    "        y_key_truth = y_key_truth + key_label_list\n",
    "\n",
    "        sequence = sequence.cuda()\n",
    "        z_out, _ = cfdet.generate(sequence, training=False)\n",
    "        z_list = torch.reshape(z_out, (-1,)).tolist()\n",
    "\n",
    "        y_key_pred = y_key_pred + z_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbHes52UkWBN",
    "outputId": "63b1f2b4-eacb-4471-a266-5c65640f8383",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9206    0.9586   2461627\n",
      "           1     0.6747    1.0000    0.8058    405573\n",
      "\n",
      "    accuracy                         0.9318   2867200\n",
      "   macro avg     0.8374    0.9603    0.8822   2867200\n",
      "weighted avg     0.9540    0.9318    0.9370   2867200\n",
      "\n",
      "[[2266084  195543]\n",
      " [      0  405573]]\n",
      "0.9602817567405623\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_key_truth, y_key_pred, digits=4))\n",
    "print(metrics.confusion_matrix(y_key_truth, y_key_pred))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_key_truth, y_key_pred, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))\n",
    "\n",
    "f = open('output.txt', 'a')\n",
    "f.write('Entry anomaly detection on detected sequences:'+'\\n')\n",
    "f.write(str(metrics.classification_report(y_key_truth, y_key_pred, digits=4))+'\\n')\n",
    "f.write(str(metrics.confusion_matrix(y_key_truth, y_key_pred))+'\\n')\n",
    "f.write(str(metrics.auc(fpr, tpr))+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:00<00:00, 89.47it/s] \n"
     ]
    }
   ],
   "source": [
    "y_key_pred2 = []\n",
    "y_key_truth2 = []\n",
    "\n",
    "\n",
    "for sequence, sequence_label, key_label in tqdm(train_test_loader3):   \n",
    "    key_label_list = torch.reshape(key_label, (-1,)).tolist()\n",
    "    y_key_truth2 = y_key_truth2 + key_label_list   \n",
    "    y_key_pred2 = y_key_pred2 + [0]*len(key_label_list)\n",
    "        \n",
    "y_key_truth_all = y_key_truth + y_key_truth2\n",
    "y_key_pred_all = y_key_pred + y_key_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9442    0.9713   3506107\n",
      "           1     0.6747    1.0000    0.8058    405573\n",
      "\n",
      "    accuracy                         0.9500   3911680\n",
      "   macro avg     0.8374    0.9721    0.8885   3911680\n",
      "weighted avg     0.9663    0.9500    0.9541   3911680\n",
      "\n",
      "[[3310564  195543]\n",
      " [      0  405573]]\n",
      "0.9721139429002025\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_key_truth_all, y_key_pred_all, digits=4))\n",
    "print(metrics.confusion_matrix(y_key_truth_all, y_key_pred_all))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_key_truth_all, y_key_pred_all, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))\n",
    "\n",
    "f = open('output.txt', 'a')\n",
    "f.write('Entry anomaly detection on unlabeled dataset:'+'\\n')\n",
    "f.write(str(metrics.classification_report(y_key_truth_all, y_key_pred_all, digits=4))+'\\n')\n",
    "f.write(str(metrics.confusion_matrix(y_key_truth_all, y_key_pred_all))+'\\n')\n",
    "f.write(str(metrics.auc(fpr, tpr))+'\\n')\n",
    "f.write('-'*50+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Rationale_Thunderbird.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
