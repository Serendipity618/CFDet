{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4qMTHDxfeA1K"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DC-DBe_qeMhi"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kjuia1oReNza"
   },
   "outputs": [],
   "source": [
    "seed = 10\n",
    "\n",
    "def setup_seed(seed=seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "     torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('output.txt', 'a')\n",
    "f.write('seed: ' + str(seed) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eAB4y1QIePRh"
   },
   "outputs": [],
   "source": [
    "logdata = pd.read_csv(r'~/Python_projects/Rationale/Dataset/BGL.log_structured_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Rk3yk9yfeQ1Z"
   },
   "outputs": [],
   "source": [
    "def slide_window(logdata, window_size = 20, step_size = 10):\n",
    "    logdata[\"Label\"] = logdata[\"Label\"].apply(lambda x: int(x != '-'))\n",
    "    data = logdata.loc[:, ['EventId', 'Label']]\n",
    "    data['Key_label'] = data['Label']\n",
    "    data.rename(columns={'Label':'Sequence_label'})\n",
    "    logkey = data['EventId']\n",
    "    logkey_label = data['Key_label']\n",
    "\n",
    "    new_data = []\n",
    "    idx = 0\n",
    "\n",
    "    while idx <= data.shape[0] - window_size:\n",
    "        new_data.append([\n",
    "                         logkey[idx : idx+window_size].values,\n",
    "                         max(logkey_label[idx : idx+window_size]),\n",
    "                         logkey_label[idx : idx+window_size].values\n",
    "                        ])\n",
    "        idx += step_size\n",
    "    return pd.DataFrame(new_data, columns = ['EventId', 'Sequence_label', 'Key_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "9jx15xtqeSZK",
    "outputId": "76e954a1-c044-4e2c-b3b7-93f3ac0e7f5b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>Sequence_label</th>\n",
       "      <th>Key_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3aa50e45, 3aa50e45, 3aa50e45, 3aa50e45, 3aa50...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3aa50e45, 3aa50e45, 3aa50e45, 3aa50e45, 3aa50...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3aa50e45, 3aa50e45, 3aa50e45, 3aa50e45, 3aa50...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[3aa50e45, 3aa50e45, 3aa50e45, 3aa50e45, 3aa50...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3aa50e45, 3aa50e45, 3aa50e45, 3aa50e45, 3aa50...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471343</th>\n",
       "      <td>[8df7ac9e, 3aa50e45, a450c390, a450c390, cfae5...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471344</th>\n",
       "      <td>[a450c390, cfae5cde, a450c390, a450c390, a450c...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471345</th>\n",
       "      <td>[a450c390, cfae5cde, 26c05abc, 26c05abc, 26c05...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471346</th>\n",
       "      <td>[a450c390, a450c390, 26c05abc, 26c05abc, 30b3b...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471347</th>\n",
       "      <td>[a450c390, a450c390, 43c21335, 30b3b946, 8df7a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>471348 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  EventId  Sequence_label  \\\n",
       "0       [3aa50e45, 3aa50e45, 3aa50e45, 3aa50e45, 3aa50...               0   \n",
       "1       [3aa50e45, 3aa50e45, 3aa50e45, 3aa50e45, 3aa50...               0   \n",
       "2       [3aa50e45, 3aa50e45, 3aa50e45, 3aa50e45, 3aa50...               0   \n",
       "3       [3aa50e45, 3aa50e45, 3aa50e45, 3aa50e45, 3aa50...               0   \n",
       "4       [3aa50e45, 3aa50e45, 3aa50e45, 3aa50e45, 3aa50...               0   \n",
       "...                                                   ...             ...   \n",
       "471343  [8df7ac9e, 3aa50e45, a450c390, a450c390, cfae5...               0   \n",
       "471344  [a450c390, cfae5cde, a450c390, a450c390, a450c...               0   \n",
       "471345  [a450c390, cfae5cde, 26c05abc, 26c05abc, 26c05...               0   \n",
       "471346  [a450c390, a450c390, 26c05abc, 26c05abc, 30b3b...               0   \n",
       "471347  [a450c390, a450c390, 43c21335, 30b3b946, 8df7a...               1   \n",
       "\n",
       "                                                Key_label  \n",
       "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                   ...  \n",
       "471343  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "471344  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "471345  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "471346  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "471347  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[471348 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = slide_window(logdata)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GLgqsB_EeT0V"
   },
   "outputs": [],
   "source": [
    "normal_ds = dataset[dataset['Sequence_label']==0]\n",
    "abnormal_ds = dataset[dataset['Sequence_label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sdZen-UwfajL"
   },
   "outputs": [],
   "source": [
    "setup_seed()\n",
    "\n",
    "train_ds, rest_ds = train_test_split(normal_ds, test_size=0.2, random_state=2021)\n",
    "test_normal_ds, val_normal_ds = train_test_split(rest_ds, test_size=0.1, random_state=2021)\n",
    "test_abnormal_ds, val_abnormal_ds = train_test_split(abnormal_ds, test_size=0.1, random_state=2021)\n",
    "\n",
    "test_ds = pd.concat([test_normal_ds, test_abnormal_ds])\n",
    "val_ds = pd.concat([val_normal_ds, val_abnormal_ds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-CC-KhPDJ8f"
   },
   "source": [
    "**2. Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KyvIiONflmX8"
   },
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "\n",
    "for index, row in train_ds.iterrows():\n",
    "    counts.update(row['EventId'])\n",
    "\n",
    "logkey2index ={\"\":0,\"UNK\":1}\n",
    "logkeys = [\"\",\"UNK\"]\n",
    "\n",
    "for word in counts:\n",
    "    logkey2index[word] = len(logkeys)\n",
    "    logkeys.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whyFN9tPlvON",
    "outputId": "2447001c-5dc7-4f04-b404-fa1d404bd110",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hecheng/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1597: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "/home/hecheng/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "def encode_sequence(sequence, logkey2index):\n",
    "    return np.array([logkey2index.get(logkey, logkey2index[\"UNK\"]) for logkey in sequence])\n",
    "\n",
    "train_ds.loc[:,'Encoded'] = train_ds.loc[:,'EventId'].apply(lambda x: encode_sequence(x,logkey2index))\n",
    "test_ds.loc[:,'Encoded'] = test_ds.loc[:,'EventId'].apply(lambda x: encode_sequence(x,logkey2index))\n",
    "val_ds.loc[:,'Encoded'] = val_ds.loc[:,'EventId'].apply(lambda x: encode_sequence(x,logkey2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1ZfMwn-dm5U5"
   },
   "outputs": [],
   "source": [
    "train_data = train_ds[['Encoded', 'Sequence_label', 'Key_label']]\n",
    "test_data = test_ds[['Encoded', 'Sequence_label', 'Key_label']]\n",
    "val_data = val_ds[['Encoded', 'Sequence_label', 'Key_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ttqZYBggnVkU"
   },
   "outputs": [],
   "source": [
    "class LogDataset(Dataset):\n",
    "    def __init__(self, sequence, sequence_label, key_label):\n",
    "        self.sequence = sequence\n",
    "        self.sequence_label = sequence_label\n",
    "        self.key_label = key_label\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return (self.sequence[idx], self.sequence_label[idx], self.key_label[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 512\n",
    "batch_size_test = 4096\n",
    "batch_size_val = 4096\n",
    "batch_size_train_test = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "fLhwhscqnjOa"
   },
   "outputs": [],
   "source": [
    "setup_seed()\n",
    "\n",
    "def dataset_dataloader(data, batch_size):\n",
    "    sequence = data['Encoded'].tolist()\n",
    "    sequence_label = data['Sequence_label'].tolist()\n",
    "    key_label = data['Key_label'].tolist()\n",
    "    dataset = LogDataset(sequence, sequence_label, key_label)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return data_loader\n",
    "\n",
    "train_loader = dataset_dataloader(train_data, batch_size = batch_size_train)\n",
    "test_loader = dataset_dataloader(test_data, batch_size = batch_size_test)\n",
    "val_loader = dataset_dataloader(val_data, batch_size = batch_size_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lf09uo30Z17N"
   },
   "source": [
    "**3. Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_mHXjMBWovar"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(logkeys)\n",
    "embedding_dim = 50\n",
    "hidden_dim = 128\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "W64xqXJKoyJe"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim=8, hidden_dim=64, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.randn(self.num_layers, x.size(0), self.hidden_dim).cuda()\n",
    "        c0 = torch.randn(self.num_layers, x.size(0), self.hidden_dim).cuda()\n",
    "\n",
    "        embedded = self.embeddings(x)\n",
    "        out, (hidden, cell) = self.lstm(embedded, (h0, c0))    \n",
    "        return torch.squeeze(torch.mean(out, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3Q2cZYJOozqi"
   },
   "outputs": [],
   "source": [
    "model = Net(vocab_size, embedding_dim, hidden_dim, num_layers).cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKpZa23Ha0hK",
    "outputId": "2e1b0ad9-2db5-4357-e73b-1b0bd6a6d4b5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  MSE:  0.0006750313881839166\n",
      "Epoch  2  MSE:  4.486155241741581e-05\n",
      "Epoch  3  MSE:  1.6476036501648532e-05\n",
      "Epoch  4  MSE:  7.75737943382291e-06\n",
      "Epoch  5  MSE:  4.023081985664273e-06\n",
      "Epoch  6  MSE:  2.2438005613166834e-06\n",
      "Epoch  7  MSE:  1.3482979122839048e-06\n",
      "Epoch  8  MSE:  8.564458152095418e-07\n",
      "Epoch  9  MSE:  5.748527161646501e-07\n",
      "Epoch  10  MSE:  3.9784477392673136e-07\n",
      "Epoch  11  MSE:  2.8490854812444174e-07\n",
      "Epoch  12  MSE:  2.1012281606332157e-07\n",
      "Epoch  13  MSE:  1.5816135890483623e-07\n",
      "Epoch  14  MSE:  1.230816358642887e-07\n",
      "Epoch  15  MSE:  9.761777269435944e-08\n",
      "Epoch  16  MSE:  7.900033083115011e-08\n",
      "Epoch  17  MSE:  6.56793031478839e-08\n",
      "Epoch  18  MSE:  5.5237490276043595e-08\n",
      "Epoch  19  MSE:  4.7822949351604334e-08\n",
      "Epoch  20  MSE:  4.176315281185951e-08\n",
      "Epoch  21  MSE:  3.7238662644491976e-08\n",
      "Epoch  22  MSE:  3.3153502832280664e-08\n",
      "Epoch  23  MSE:  3.010923215556097e-08\n",
      "Epoch  24  MSE:  2.778041587029638e-08\n",
      "Epoch  25  MSE:  2.47663711563033e-08\n",
      "Epoch  26  MSE:  2.357312841213278e-08\n",
      "Epoch  27  MSE:  2.1300691906036072e-08\n",
      "Epoch  28  MSE:  2.033240679668713e-08\n",
      "Epoch  29  MSE:  1.9466773218331625e-08\n",
      "Epoch  30  MSE:  1.7650003548180712e-08\n",
      "Epoch  31  MSE:  1.6800727155861767e-08\n",
      "Epoch  32  MSE:  1.6003864729726436e-08\n",
      "Epoch  33  MSE:  1.637144830136425e-08\n",
      "Epoch  34  MSE:  1.4496538725907832e-08\n",
      "Epoch  35  MSE:  1.4101267385071962e-08\n",
      "Epoch  36  MSE:  1.3375170610158955e-08\n",
      "Epoch  37  MSE:  1.3347482842940493e-08\n",
      "Epoch  38  MSE:  1.2684963698557692e-08\n",
      "Epoch  39  MSE:  1.219634063810339e-08\n",
      "Epoch  40  MSE:  1.2588184413954905e-08\n",
      "Epoch  41  MSE:  1.1102589164693834e-08\n",
      "Epoch  42  MSE:  1.2150797529773053e-08\n",
      "Epoch  43  MSE:  1.083539283458361e-08\n",
      "Epoch  44  MSE:  1.019346343374805e-08\n",
      "Epoch  45  MSE:  1.0650006301316853e-08\n",
      "Epoch  46  MSE:  9.73495562649396e-09\n",
      "Epoch  47  MSE:  9.935018805988922e-09\n",
      "Epoch  48  MSE:  9.518832178480251e-09\n",
      "Epoch  49  MSE:  9.99552637225688e-09\n",
      "Epoch  50  MSE:  9.091388647027011e-09\n"
     ]
    }
   ],
   "source": [
    "# if not os.path.exists('DeepSVDD.bin'):\n",
    "setup_seed()\n",
    "\n",
    "epochs = 50\n",
    "total_loss = []\n",
    "r_candidate = []\n",
    "dist_list = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    epoch_loss=[]\n",
    "    hidden_sum = torch.zeros((batch_size_train, hidden_dim))\n",
    "\n",
    "    if i < 20:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for sequence, sequence_label, _ in train_loader:\n",
    "                if len(sequence_label) == batch_size_train:\n",
    "                    sequence = sequence.cuda()\n",
    "                    hidden_sum = hidden_sum.cuda()\n",
    "                    hidden1 = model(sequence)\n",
    "                    hidden_sum = hidden_sum + hidden1\n",
    "                    sequence = sequence.cpu()\n",
    "\n",
    "\n",
    "        center = (torch.mean(hidden_sum.cuda(), axis=0) / len(train_loader))\n",
    "        center_batch = torch.repeat_interleave(torch.unsqueeze(center, 0), batch_size_train, dim=0).detach()\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    for sequence2, sequence_label2, _ in train_loader:\n",
    "        if len(sequence_label2) == batch_size_train:\n",
    "            sequence2 = sequence2.cuda()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            hidden2 = model(sequence2)  \n",
    "            loss = criterion(hidden2, center_batch.cuda())  \n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "#             if i == epochs-1:\n",
    "#                 r_candidate.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Epoch \", i+1, \" MSE: \", np.mean(epoch_loss))\n",
    "    total_loss.append(np.max(epoch_loss))\n",
    "#         if total_loss[i] < min_loss:\n",
    "    if i == epochs-1:\n",
    "        torch.save(model.state_dict(), './DeepSVDD.bin')\n",
    "        min_loss = total_loss[i]\n",
    "        r = total_loss[i]\n",
    "\n",
    "        f = open('center_radius.txt', 'w+')\n",
    "        f.write(str(center.tolist()))\n",
    "        f.write('\\n')\n",
    "        f.write(str(r))\n",
    "        f.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('DeepSVDD.bin'))\n",
    "\n",
    "f = open('center_radius.txt','r')\n",
    "center_radius = f.readlines()\n",
    "f.close()\n",
    "\n",
    "center = torch.tensor(eval(center_radius[0])).cuda()\n",
    "r = eval(center_radius[1])\n",
    "\n",
    "y_pred = []\n",
    "y_truth = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence, sequence_label, _ in val_loader: \n",
    "        y_truth = y_truth + sequence_label.tolist()\n",
    "        sequence = sequence.cuda()\n",
    "        hidden = model(sequence)\n",
    "        distance = torch.mean(torch.square(hidden-center), dim=1)\n",
    "        y_pred_batch = [int(i>r) for i in distance]\n",
    "        y_pred = y_pred + y_pred_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GK2i7KyGpHbf",
    "outputId": "1c339d60-37dd-494c-a884-d9363f9e498f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9932    0.9935    0.9933      8617\n",
      "           1     0.9862    0.9854    0.9858      4053\n",
      "\n",
      "    accuracy                         0.9909     12670\n",
      "   macro avg     0.9897    0.9895    0.9896     12670\n",
      "weighted avg     0.9909    0.9909    0.9909     12670\n",
      "\n",
      "[[8561   56]\n",
      " [  59 3994]]\n",
      "0.989472050168733\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_truth, y_pred, digits=4))\n",
    "print(metrics.confusion_matrix(y_truth, y_pred))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_truth, y_pred, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))\n",
    "\n",
    "f = open('output.txt', 'a')\n",
    "f.write('Sequence anomaly detection: '+'\\n')\n",
    "f.write(str(metrics.classification_report(y_truth, y_pred, digits=4))+'\\n')\n",
    "f.write(str(metrics.confusion_matrix(y_truth, y_pred))+'\\n')\n",
    "f.write(str(metrics.auc(fpr, tpr))+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('DeepSVDD.bin'))\n",
    "\n",
    "f = open('center_radius.txt','r')\n",
    "center_radius = f.readlines()\n",
    "f.close()\n",
    "\n",
    "center = torch.tensor(eval(center_radius[0])).cuda()\n",
    "r = eval(center_radius[1])\n",
    "\n",
    "y_pred = []\n",
    "y_truth = []\n",
    "seq_list = []\n",
    "distance_list = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence, sequence_label, _ in train_loader: \n",
    "        y_truth = y_truth + sequence_label.tolist()\n",
    "        seq_list += sequence.tolist()\n",
    "        sequence = sequence.cuda()\n",
    "        hidden = model(sequence)\n",
    "        distance = torch.mean(torch.square(hidden-center), dim=1)\n",
    "        distance_list += distance.tolist()\n",
    "        y_pred_batch = [int(i>r) for i in distance]\n",
    "        y_pred = y_pred + y_pred_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 14, 14,  4,  4,  4,  4, 14, 14,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4, 14], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_sequence = torch.tensor(seq_list[np.argmin(distance_list)]).to(device)\n",
    "baseline_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GSbFooWvkWBK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "sequence_list = []\n",
    "sequence_label_list = []\n",
    "key_label_list = []\n",
    "\n",
    "sequence_list2 = []\n",
    "sequence_label_list2 = []\n",
    "key_label_list2 = []\n",
    "\n",
    "sequence_list3 = []\n",
    "sequence_label_list3 = []\n",
    "key_label_list3 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence, sequence_label, key_label in test_loader: \n",
    "        sequence = sequence.cuda()\n",
    "        \n",
    "        hidden = model(sequence)\n",
    "        distance = torch.mean(torch.square(hidden-center), dim=1)\n",
    "        y_pred_index_batch = [i for i in range(len(distance)) if distance[i]>10*r]\n",
    "        y_pred_index_batch2 = [i for i in range(len(distance)) if distance[i]>r]\n",
    "        y_pred_index_batch3 = [i for i in range(len(distance)) if distance[i]<=r]\n",
    "        \n",
    "        sequence_l = sequence.tolist()\n",
    "        sequence_label_l = sequence_label.tolist()\n",
    "        key_label_l = key_label.tolist()\n",
    "        \n",
    "        for i in y_pred_index_batch:\n",
    "            sequence_list += [sequence_l[i]]\n",
    "            sequence_label_list += [sequence_label_l[i]]\n",
    "            key_label_list += [key_label_l[i]]\n",
    "            \n",
    "        for j in y_pred_index_batch2:\n",
    "            sequence_list2 += [sequence_l[j]]\n",
    "            sequence_label_list2 += [sequence_label_l[j]]\n",
    "            key_label_list2 += [key_label_l[j]]\n",
    "            \n",
    "        for k in y_pred_index_batch3:\n",
    "            sequence_list3 += [sequence_l[k]]\n",
    "            sequence_label_list3 += [sequence_label_l[k]]\n",
    "            key_label_list3 += [key_label_l[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "aalNggSKkWBK"
   },
   "outputs": [],
   "source": [
    "def train_test_data_loader(sequence_list, sequence_label_list, key_label_list):\n",
    "    d = {'Encoded': sequence_list,\n",
    "         'Sequence_label': sequence_label_list,\n",
    "         'Key_label': key_label_list}\n",
    "\n",
    "    train_test_data = pd.DataFrame(d)\n",
    "\n",
    "    train_test_data['Encoded'] = [torch.tensor(i) for i in train_test_data['Encoded']]\n",
    "    train_test_data['Sequence_label'] = [torch.tensor(i) for i in train_test_data['Sequence_label']]\n",
    "    train_test_data['Key_label'] = [torch.tensor(i) for i in train_test_data['Key_label']]\n",
    "\n",
    "    train_test_loader = dataset_dataloader(train_test_data, batch_size = batch_size_train_test)\n",
    "    return train_test_loader, train_test_data\n",
    "\n",
    "train_test_loader, train_test_data   = train_test_data_loader(sequence_list, sequence_label_list, key_label_list)\n",
    "train_test_loader2, train_test_data2 = train_test_data_loader(sequence_list2, sequence_label_list2, key_label_list2)\n",
    "train_test_loader3, train_test_data3 = train_test_data_loader(sequence_list3, sequence_label_list3, key_label_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "wWxEAjsykWBK"
   },
   "outputs": [],
   "source": [
    "embedding_dim2 = 100\n",
    "hidden_dim2 = 128\n",
    "num_layers2 = 1\n",
    "triplet_lambda = 1\n",
    "continuity_lambda = 0.05\n",
    "sparsity_lambda = 0.15\n",
    "epochs2 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=256, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embeddings(x)\n",
    "        out, (hidden, cell) = self.lstm(embedded)    \n",
    "        scores = self.output_layer(out) \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "D93Iozz-kWBL"
   },
   "outputs": [],
   "source": [
    "class CFDet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CFDet, self).__init__()\n",
    "        self.exploration_rate = 0.05\n",
    "        self.count_tokens = 3\n",
    "        self.count_pieces = 3\n",
    "        self.generator = Generator(vocab_size, embedding_dim2, hidden_dim2, num_layers2).cuda()\n",
    "\n",
    "    def generate(self, x, training=True):\n",
    "        z_scores_ = self.generator(x)\n",
    "        z_probs_ = F.softmax(z_scores_, dim=-1)\n",
    "        z_prob_ = (1 - self.exploration_rate) * z_probs_ + self.exploration_rate / z_probs_.size(-1)\n",
    "        z_prob__ = z_prob_.view(-1, 2)\n",
    "        sampler = torch.distributions.Categorical(z_prob__)\n",
    "\n",
    "        if training:\n",
    "            z_ = sampler.sample()  # (num_rows * p_length,)\n",
    "            z = z_.view(z_prob_.size(0), z_prob_.size(1))\n",
    "            z = z.type(torch.cuda.IntTensor)\n",
    "            neg_log_probs_ = -sampler.log_prob(z_)\n",
    "            neg_log_probs = neg_log_probs_.view(z_prob_.size(0), z_prob_.size(1))\n",
    "            return z, neg_log_probs\n",
    "        else:\n",
    "            z__index = torch.max(z_prob__, dim=-1)[1]\n",
    "            z0 = z__index.view(z_prob_.size(0), z_prob_.size(1))\n",
    "            z_index = z0.type(torch.cuda.IntTensor)\n",
    "\n",
    "            z__value = torch.max(z_prob__, dim=-1)[0]\n",
    "            # z1 = z__value.view(z_prob_.size(0), z_prob_.size(1))\n",
    "            z_value = z__value.type(torch.cuda.FloatTensor)\n",
    "            return z_index, z_value\n",
    "\n",
    "    def get_loss(self, x, z, neg_log_probs,average_reward, batch_size, model, sequence_length=20.0):\n",
    "        z_ = torch.cat([z[:, 1:], z[:, -1:]], dim=-1)\n",
    "        continuity_ratio = torch.div(torch.sum(torch.abs(z - z_), dim=-1), sequence_length)\n",
    "        percentage = (self.count_pieces-1) / sequence_length\n",
    "        continuity_loss = torch.abs(continuity_ratio - percentage)\n",
    "#         continuity_loss = torch.clamp(continuity_ratio - percentage, min=0)\n",
    "\n",
    "\n",
    "        sparsity_ratio = torch.div(torch.sum(z, dim=-1), sequence_length)\n",
    "        percentage = self.count_tokens / sequence_length\n",
    "        sparsity_loss = torch.abs(sparsity_ratio - percentage)\n",
    "#         sparsity_loss = torch.clamp(sparsity_ratio - percentage, min=0)\n",
    "        \n",
    "        anomalous_entry = x * z + baseline_sequence * (1-z)\n",
    "        anti = x * (1-z) + baseline_sequence * z\n",
    "        hidden_anomalous_entry = model(anomalous_entry)\n",
    "        hidden_anti = model(anti)\n",
    "        distance_loss = criterion2(center_batch2, hidden_anti, hidden_anomalous_entry) + criterion(center_batch2, hidden_anti) \\\n",
    "                        - criterion(center_batch2, hidden_anomalous_entry)\n",
    "\n",
    "        average_reward = average_reward.cuda()\n",
    "        rewards = -(triplet_lambda * distance_loss + sparsity_lambda * sparsity_loss + continuity_lambda * continuity_loss ).detach()\n",
    "        advantages = rewards - average_reward # (batch_size,)\n",
    "\n",
    "        advantages_expand_ = advantages.unsqueeze(-1).expand_as(neg_log_probs)       \n",
    "        rl_loss = torch.sum(neg_log_probs * advantages_expand_)\n",
    "        \n",
    "        return distance_loss, rl_loss, rewards, continuity_loss, sparsity_loss, advantages_expand_\n",
    "\n",
    "    def training_step(self, distance_loss, rl_loss):\n",
    "        rl_loss.backward()\n",
    "        optimiser2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Hj_YWgagkWBL"
   },
   "outputs": [],
   "source": [
    "cfdet = CFDet()\n",
    "criterion2 = nn.TripletMarginLoss(margin=1, reduction='none')\n",
    "optimiser2 = optim.Adam(cfdet.generator.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z859sJLQkWBM",
    "outputId": "46554397-351f-4c88-830c-b5e5cfba80c5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1:\n",
      "distance_loss: 0.31647391617298126 continuity loss:  0.13837333066122873 sparsity loss:  0.70412945662226\n",
      "------------------------------------------------------\n",
      "epoch2:\n",
      "distance_loss: 0.19097995374883925 continuity loss:  0.08020089460270745 sparsity loss:  0.7961872049740383\n",
      "------------------------------------------------------\n",
      "epoch3:\n",
      "distance_loss: 0.18916069992950985 continuity loss:  0.08035435421126229 sparsity loss:  0.7887472067560468\n",
      "------------------------------------------------------\n",
      "epoch4:\n",
      "distance_loss: 0.18876558542251587 continuity loss:  0.07988002406699317 sparsity loss:  0.78573659658432\n",
      "------------------------------------------------------\n",
      "epoch5:\n",
      "distance_loss: 0.18856621895517622 continuity loss:  0.07832310327461788 sparsity loss:  0.7880064146859306\n",
      "------------------------------------------------------\n",
      "epoch6:\n",
      "distance_loss: 0.18816181719303132 continuity loss:  0.07716239073446819 sparsity loss:  0.7856780001095363\n",
      "------------------------------------------------------\n",
      "epoch7:\n",
      "distance_loss: 0.1884532481431961 continuity loss:  0.07647600706134523 sparsity loss:  0.7848088588033404\n",
      "------------------------------------------------------\n",
      "epoch8:\n",
      "distance_loss: 0.18924429076058524 continuity loss:  0.08635463459151131 sparsity loss:  0.7674427952085222\n",
      "------------------------------------------------------\n",
      "epoch9:\n",
      "distance_loss: 0.18956886913095203 continuity loss:  0.11220145395823887 sparsity loss:  0.7433830823217119\n",
      "------------------------------------------------------\n",
      "epoch10:\n",
      "distance_loss: 0.18918860001223428 continuity loss:  0.11906250289508274 sparsity loss:  0.7344670602253505\n",
      "------------------------------------------------------\n",
      "epoch11:\n",
      "distance_loss: 0.18924826341015952 continuity loss:  0.11578962206840515 sparsity loss:  0.7292647787502834\n",
      "------------------------------------------------------\n",
      "epoch12:\n",
      "distance_loss: 0.18968054183891842 continuity loss:  0.12246094069310597 sparsity loss:  0.7201213700430734\n",
      "------------------------------------------------------\n",
      "epoch13:\n",
      "distance_loss: 0.18825597550187792 continuity loss:  0.13216936716011593 sparsity loss:  0.7192884990147181\n",
      "------------------------------------------------------\n",
      "epoch14:\n",
      "distance_loss: 0.1894611975976399 continuity loss:  0.13693778089114597 sparsity loss:  0.7117410659790039\n",
      "------------------------------------------------------\n",
      "epoch15:\n",
      "distance_loss: 0.18849063260214668 continuity loss:  0.13383231290749142 sparsity loss:  0.7113253286906651\n",
      "------------------------------------------------------\n",
      "epoch16:\n",
      "distance_loss: 0.18905006561960494 continuity loss:  0.13338867468493326 sparsity loss:  0.710039050238473\n",
      "------------------------------------------------------\n",
      "epoch17:\n",
      "distance_loss: 0.18872093600886208 continuity loss:  0.13100586043936865 sparsity loss:  0.7073214224406651\n",
      "------------------------------------------------------\n",
      "epoch18:\n",
      "distance_loss: 0.19175502232142858 continuity loss:  0.11128767026322228 sparsity loss:  0.6965638927050999\n",
      "------------------------------------------------------\n",
      "epoch19:\n",
      "distance_loss: 0.18966599958283561 continuity loss:  0.1261858297245843 sparsity loss:  0.704296863079071\n",
      "------------------------------------------------------\n",
      "epoch20:\n",
      "distance_loss: 0.19045425227710178 continuity loss:  0.11451869649546487 sparsity loss:  0.6995535663196019\n",
      "------------------------------------------------------\n",
      "epoch21:\n",
      "distance_loss: 0.18845433805670056 continuity loss:  0.12454938931124551 sparsity loss:  0.7099511640412467\n",
      "------------------------------------------------------\n",
      "epoch22:\n",
      "distance_loss: 0.1878521578652518 continuity loss:  0.1212011741740363 sparsity loss:  0.71682197366442\n",
      "------------------------------------------------------\n",
      "epoch23:\n",
      "distance_loss: 0.18835191896983555 continuity loss:  0.11809430995157787 sparsity loss:  0.7144294023513794\n",
      "------------------------------------------------------\n",
      "epoch24:\n",
      "distance_loss: 0.1886645793914795 continuity loss:  0.11831333956548146 sparsity loss:  0.7137179102216448\n",
      "------------------------------------------------------\n",
      "epoch25:\n",
      "distance_loss: 0.18794895325388228 continuity loss:  0.1222154044679233 sparsity loss:  0.71228654725211\n",
      "------------------------------------------------------\n",
      "epoch26:\n",
      "distance_loss: 0.18786173846040452 continuity loss:  0.1261830380984715 sparsity loss:  0.7084123815808977\n",
      "------------------------------------------------------\n",
      "epoch27:\n",
      "distance_loss: 0.1887973027569907 continuity loss:  0.12073242366313934 sparsity loss:  0.7056389468056815\n",
      "------------------------------------------------------\n",
      "epoch28:\n",
      "distance_loss: 0.18895154893398286 continuity loss:  0.11209542602300644 sparsity loss:  0.7090359943253653\n",
      "------------------------------------------------------\n",
      "epoch29:\n",
      "distance_loss: 0.18748024318899428 continuity loss:  0.11804408665214267 sparsity loss:  0.7137988209724426\n",
      "------------------------------------------------------\n",
      "epoch30:\n",
      "distance_loss: 0.18866254218987055 continuity loss:  0.11753488119159426 sparsity loss:  0.7097488829067775\n",
      "------------------------------------------------------\n",
      "epoch31:\n",
      "distance_loss: 0.19198810841356004 continuity loss:  0.09787109621933528 sparsity loss:  0.7023897750037057\n",
      "------------------------------------------------------\n",
      "epoch32:\n",
      "distance_loss: 0.1892954409122467 continuity loss:  0.11102957831961768 sparsity loss:  0.7093959246362959\n",
      "------------------------------------------------------\n",
      "epoch33:\n",
      "distance_loss: 0.1890984582049506 continuity loss:  0.11612165357385362 sparsity loss:  0.713035706111363\n",
      "------------------------------------------------------\n",
      "epoch34:\n",
      "distance_loss: 0.18871737377984182 continuity loss:  0.11883789151906968 sparsity loss:  0.7120577556746347\n",
      "------------------------------------------------------\n",
      "epoch35:\n",
      "distance_loss: 0.1883120983839035 continuity loss:  0.12131975718906947 sparsity loss:  0.7113894973482404\n",
      "------------------------------------------------------\n",
      "epoch36:\n",
      "distance_loss: 0.18803223073482514 continuity loss:  0.12028320516858781 sparsity loss:  0.71236606495721\n",
      "------------------------------------------------------\n",
      "epoch37:\n",
      "distance_loss: 0.18809233009815216 continuity loss:  0.11926897657769067 sparsity loss:  0.7121986525399344\n",
      "------------------------------------------------------\n",
      "epoch38:\n",
      "distance_loss: 0.1877390431506293 continuity loss:  0.1196414651615279 sparsity loss:  0.7117061887468611\n",
      "------------------------------------------------------\n",
      "epoch39:\n",
      "distance_loss: 0.188062926701137 continuity loss:  0.12664481145995005 sparsity loss:  0.7073255981717791\n",
      "------------------------------------------------------\n",
      "epoch40:\n",
      "distance_loss: 0.1880714706012181 continuity loss:  0.12953962300504956 sparsity loss:  0.7057840296200344\n",
      "------------------------------------------------------\n",
      "epoch41:\n",
      "distance_loss: 0.18761774216379437 continuity loss:  0.13122907621519905 sparsity loss:  0.7061914035252163\n",
      "------------------------------------------------------\n",
      "epoch42:\n",
      "distance_loss: 0.18800254123551505 continuity loss:  0.1300851027880396 sparsity loss:  0.7049902319908142\n",
      "------------------------------------------------------\n",
      "epoch43:\n",
      "distance_loss: 0.1881215806518282 continuity loss:  0.13028320563691004 sparsity loss:  0.7048339792660304\n",
      "------------------------------------------------------\n",
      "epoch44:\n",
      "distance_loss: 0.18822934712682451 continuity loss:  0.13018275925091335 sparsity loss:  0.704670752797808\n",
      "------------------------------------------------------\n",
      "epoch45:\n",
      "distance_loss: 0.18845065491540092 continuity loss:  0.12914341517857142 sparsity loss:  0.7031808018684387\n",
      "------------------------------------------------------\n",
      "epoch46:\n",
      "distance_loss: 0.189023334639413 continuity loss:  0.12907784879207612 sparsity loss:  0.701805237361363\n",
      "------------------------------------------------------\n",
      "epoch47:\n",
      "distance_loss: 0.1892996587923595 continuity loss:  0.1287555830819266 sparsity loss:  0.7003794653075082\n",
      "------------------------------------------------------\n",
      "epoch48:\n",
      "distance_loss: 0.18854185257639203 continuity loss:  0.13027622997760774 sparsity loss:  0.7032742687634059\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch49:\n",
      "distance_loss: 0.19002125007765633 continuity loss:  0.12858119819845473 sparsity loss:  0.7001241632870265\n",
      "------------------------------------------------------\n",
      "epoch50:\n",
      "distance_loss: 0.18750711168561662 continuity loss:  0.12968471241848809 sparsity loss:  0.7038936887468611\n",
      "------------------------------------------------------\n",
      "epoch51:\n",
      "distance_loss: 0.18779342515128 continuity loss:  0.12920061626604626 sparsity loss:  0.7037374360220773\n",
      "------------------------------------------------------\n",
      "epoch52:\n",
      "distance_loss: 0.18723048397472927 continuity loss:  0.13010742494038174 sparsity loss:  0.704335926260267\n",
      "------------------------------------------------------\n",
      "epoch53:\n",
      "distance_loss: 0.1879486037152154 continuity loss:  0.1311481575880732 sparsity loss:  0.7041936312402998\n",
      "------------------------------------------------------\n",
      "epoch54:\n",
      "distance_loss: 0.18790400453976222 continuity loss:  0.13166155474526542 sparsity loss:  0.7042327097484044\n",
      "------------------------------------------------------\n",
      "epoch55:\n",
      "distance_loss: 0.1881469977753503 continuity loss:  0.13125418956790652 sparsity loss:  0.7041810716901507\n",
      "------------------------------------------------------\n",
      "epoch56:\n",
      "distance_loss: 0.18760791165488108 continuity loss:  0.13249163095440183 sparsity loss:  0.7052301900727408\n",
      "------------------------------------------------------\n",
      "epoch57:\n",
      "distance_loss: 0.18739937160696302 continuity loss:  0.13260463497468403 sparsity loss:  0.7071609820638384\n",
      "------------------------------------------------------\n",
      "epoch58:\n",
      "distance_loss: 0.1876726691211973 continuity loss:  0.13266601881810597 sparsity loss:  0.7076199650764465\n",
      "------------------------------------------------------\n",
      "epoch59:\n",
      "distance_loss: 0.18784560561180114 continuity loss:  0.13222098456961767 sparsity loss:  0.7081542883600508\n",
      "------------------------------------------------------\n",
      "epoch60:\n",
      "distance_loss: 0.18757227531501225 continuity loss:  0.13218331720147813 sparsity loss:  0.7081082497324263\n",
      "------------------------------------------------------\n",
      "epoch61:\n",
      "distance_loss: 0.1873245916196278 continuity loss:  0.13342634290456773 sparsity loss:  0.7092661806515285\n",
      "------------------------------------------------------\n",
      "epoch62:\n",
      "distance_loss: 0.1875487587281636 continuity loss:  0.1329157420567104 sparsity loss:  0.7085281797817775\n",
      "------------------------------------------------------\n",
      "epoch63:\n",
      "distance_loss: 0.18739755536828723 continuity loss:  0.13358677689518247 sparsity loss:  0.7089480910982404\n",
      "------------------------------------------------------\n",
      "epoch64:\n",
      "distance_loss: 0.1874814829656056 continuity loss:  0.1330733835697174 sparsity loss:  0.7085030623844691\n",
      "------------------------------------------------------\n",
      "epoch65:\n",
      "distance_loss: 0.18767415285110473 continuity loss:  0.13256278336048127 sparsity loss:  0.7074804527418954\n",
      "------------------------------------------------------\n",
      "epoch66:\n",
      "distance_loss: 0.18727088229996816 continuity loss:  0.13269810527563095 sparsity loss:  0.7077427421297345\n",
      "------------------------------------------------------\n",
      "epoch67:\n",
      "distance_loss: 0.1876196712255478 continuity loss:  0.13111188943896976 sparsity loss:  0.7064955268587385\n",
      "------------------------------------------------------\n",
      "epoch68:\n",
      "distance_loss: 0.18782709836959838 continuity loss:  0.13028180982385362 sparsity loss:  0.7055691940443857\n",
      "------------------------------------------------------\n",
      "epoch69:\n",
      "distance_loss: 0.1877159263406481 continuity loss:  0.13058315089770725 sparsity loss:  0.7059068015643528\n",
      "------------------------------------------------------\n",
      "epoch70:\n",
      "distance_loss: 0.18757047653198242 continuity loss:  0.1301116093993187 sparsity loss:  0.7057156784193857\n",
      "------------------------------------------------------\n",
      "epoch71:\n",
      "distance_loss: 0.18780573308467866 continuity loss:  0.13071149800504958 sparsity loss:  0.704457300049918\n",
      "------------------------------------------------------\n",
      "epoch72:\n",
      "distance_loss: 0.18759492720876422 continuity loss:  0.130157647388322 sparsity loss:  0.7048172286578587\n",
      "------------------------------------------------------\n",
      "epoch73:\n",
      "distance_loss: 0.18774849687303816 continuity loss:  0.12976423289094652 sparsity loss:  0.7064997128077916\n",
      "------------------------------------------------------\n",
      "epoch74:\n",
      "distance_loss: 0.18777810760906766 continuity loss:  0.12785435297659464 sparsity loss:  0.708108253138406\n",
      "------------------------------------------------------\n",
      "epoch75:\n",
      "distance_loss: 0.1873896062374115 continuity loss:  0.1257673020873751 sparsity loss:  0.7098939640181405\n",
      "------------------------------------------------------\n",
      "epoch76:\n",
      "distance_loss: 0.1870918205806187 continuity loss:  0.11345284708908626 sparsity loss:  0.7207003235816956\n",
      "------------------------------------------------------\n",
      "epoch77:\n",
      "distance_loss: 0.18708823238100325 continuity loss:  0.11475586209978376 sparsity loss:  0.7213392887796675\n",
      "------------------------------------------------------\n",
      "epoch78:\n",
      "distance_loss: 0.1870982506445476 continuity loss:  0.11869419813156128 sparsity loss:  0.7319489342825753\n",
      "------------------------------------------------------\n",
      "epoch79:\n",
      "distance_loss: 0.18674493048872268 continuity loss:  0.11314313858747482 sparsity loss:  0.7393429057938712\n",
      "------------------------------------------------------\n",
      "epoch80:\n",
      "distance_loss: 0.1872619569301605 continuity loss:  0.11327427689518248 sparsity loss:  0.7361858265740531\n",
      "------------------------------------------------------\n",
      "epoch81:\n",
      "distance_loss: 0.18758191636630467 continuity loss:  0.11846819392272405 sparsity loss:  0.7255357112203326\n",
      "------------------------------------------------------\n",
      "epoch82:\n",
      "distance_loss: 0.18751810448510306 continuity loss:  0.117640905720847 sparsity loss:  0.7172363247190203\n",
      "------------------------------------------------------\n",
      "epoch83:\n",
      "distance_loss: 0.18765318606581008 continuity loss:  0.12062918777976717 sparsity loss:  0.7127636705126081\n",
      "------------------------------------------------------\n",
      "epoch84:\n",
      "distance_loss: 0.18776434659957886 continuity loss:  0.12016741314104624 sparsity loss:  0.7140917880194527\n",
      "------------------------------------------------------\n",
      "epoch85:\n",
      "distance_loss: 0.18764400226729258 continuity loss:  0.12246931024960109 sparsity loss:  0.7121149454798017\n",
      "------------------------------------------------------\n",
      "epoch86:\n",
      "distance_loss: 0.1878970273903438 continuity loss:  0.12346819468906947 sparsity loss:  0.711573656967708\n",
      "------------------------------------------------------\n",
      "epoch87:\n",
      "distance_loss: 0.18794519901275636 continuity loss:  0.12583984562328884 sparsity loss:  0.7084123764719282\n",
      "------------------------------------------------------\n",
      "epoch88:\n",
      "distance_loss: 0.1874555685690471 continuity loss:  0.12353515795298985 sparsity loss:  0.7105440803936549\n",
      "------------------------------------------------------\n",
      "epoch89:\n",
      "distance_loss: 0.18826639567102704 continuity loss:  0.12494698848043169 sparsity loss:  0.709386146068573\n",
      "------------------------------------------------------\n",
      "epoch90:\n",
      "distance_loss: 0.18765134896550859 continuity loss:  0.12956891890083042 sparsity loss:  0.7042299117360796\n",
      "------------------------------------------------------\n",
      "epoch91:\n",
      "distance_loss: 0.18772997089794705 continuity loss:  0.12895089451755795 sparsity loss:  0.7040429643222264\n",
      "------------------------------------------------------\n",
      "epoch92:\n",
      "distance_loss: 0.18816533258983068 continuity loss:  0.1295800817864282 sparsity loss:  0.7035463162830897\n",
      "------------------------------------------------------\n",
      "epoch93:\n",
      "distance_loss: 0.18764829209872655 continuity loss:  0.1291531835283552 sparsity loss:  0.7038002167429243\n",
      "------------------------------------------------------\n",
      "epoch94:\n",
      "distance_loss: 0.1890860161611012 continuity loss:  0.1163434739623751 sparsity loss:  0.697901781967708\n",
      "------------------------------------------------------\n",
      "epoch95:\n",
      "distance_loss: 0.19192655895437513 continuity loss:  0.09983677672488349 sparsity loss:  0.6930677958897182\n",
      "------------------------------------------------------\n",
      "epoch96:\n",
      "distance_loss: 0.19270307719707488 continuity loss:  0.09849190967423575 sparsity loss:  0.6940290110451834\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch97:\n",
      "distance_loss: 0.19196880757808685 continuity loss:  0.09847377389669418 sparsity loss:  0.6941406181880406\n",
      "------------------------------------------------------\n",
      "epoch98:\n",
      "distance_loss: 0.1924050177846636 continuity loss:  0.09925223333495004 sparsity loss:  0.6945256693022591\n",
      "------------------------------------------------------\n",
      "epoch99:\n",
      "distance_loss: 0.19230988281113762 continuity loss:  0.09938755674021585 sparsity loss:  0.6941447990281241\n",
      "------------------------------------------------------\n",
      "epoch100:\n",
      "distance_loss: 0.19183919600078037 continuity loss:  0.09984793769461768 sparsity loss:  0.6947795748710632\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# if not os.path.exists('state_dict_minloss.bin'):\n",
    "\n",
    "setup_seed()\n",
    "\n",
    "total_loss_list = []\n",
    "distance_loss_list = []\n",
    "reward_list = []\n",
    "continuity_loss_list = []\n",
    "sparsity_loss_list = []\n",
    "loss_list = []\n",
    "\n",
    "min_loss= 10e6\n",
    "\n",
    "cfdet.generator.train()\n",
    "model.train()\n",
    "\n",
    "center_batch2 = torch.repeat_interleave(torch.unsqueeze(center, 0), batch_size_train_test, dim=0).cuda()\n",
    "\n",
    "for i in range(epochs2):\n",
    "    z_history_rewards = deque(maxlen=200)\n",
    "    z_history_rewards.append(0.0)\n",
    "    epoch_distance_loss = []\n",
    "    epoch_continuity_loss = []\n",
    "    epoch_sparsity_loss = []\n",
    "    epoch_rl_loss = []\n",
    "    epoch_reward = []\n",
    "    epoch_loss= []\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False \n",
    "\n",
    "    for sequence4, sequence_label4, _ in train_test_loader:\n",
    "        sequence4 = sequence4.cuda()\n",
    "\n",
    "        baseline = Variable(torch.FloatTensor([float(np.mean(z_history_rewards))]))\n",
    "\n",
    "        if len(sequence_label4) == batch_size_train_test:             \n",
    "            optimiser2.zero_grad()\n",
    "\n",
    "            z, neg_log_probs = cfdet.generate(sequence4)\n",
    "            distance_loss, rl_loss, rewards, continuity_loss, sparsity_loss, advantage = cfdet.get_loss(sequence4, z, neg_log_probs, baseline, batch_size_train_test, model)\n",
    "            cfdet.training_step(distance_loss, rl_loss)\n",
    "\n",
    "            epoch_distance_loss.append(torch.mean(distance_loss).item())\n",
    "            epoch_continuity_loss.append(torch.mean(continuity_loss).item())\n",
    "            epoch_sparsity_loss.append(torch.mean(sparsity_loss).item())\n",
    "            epoch_rl_loss.append(rl_loss.item())\n",
    "            epoch_reward.append(torch.sum(rewards).item())\n",
    "            epoch_loss.append(torch.sum(-rewards).item())\n",
    "\n",
    "            z_batch_reward = np.mean(rewards.cpu().data.numpy())\n",
    "            z_history_rewards.append(z_batch_reward)\n",
    "\n",
    "    total_loss_list.append(np.mean(epoch_rl_loss))\n",
    "    continuity_loss_list.append(np.mean(epoch_continuity_loss))\n",
    "    sparsity_loss_list.append(np.mean(epoch_sparsity_loss))\n",
    "    distance_loss_list.append(np.mean(epoch_distance_loss))\n",
    "    reward_list.append(np.mean(epoch_reward))\n",
    "    loss_list.append(np.mean(epoch_loss))\n",
    "\n",
    "    if distance_loss_list[i] + continuity_lambda * continuity_loss_list[i] + sparsity_lambda * sparsity_loss_list[i] < min_loss:\n",
    "        min_loss = distance_loss_list[i] + continuity_lambda * continuity_loss_list[i] + sparsity_lambda * sparsity_loss_list[i]\n",
    "        torch.save(cfdet.generator.state_dict(), './state_dict_minloss.bin')\n",
    "    if i == epochs2-1:\n",
    "        torch.save(cfdet.generator.state_dict(), './state_dict_final.bin')\n",
    "\n",
    "    print(f'epoch{i+1}:')\n",
    "    print('distance_loss:', distance_loss_list[i], 'continuity loss: ', continuity_loss_list[i], 'sparsity loss: ', sparsity_loss_list[i])\n",
    "    print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "QJ_yCy1-kWBM"
   },
   "outputs": [],
   "source": [
    "cfdet.generator.load_state_dict(torch.load('state_dict_minloss.bin')) \n",
    "\n",
    "y_key_pred2 = []\n",
    "y_key_truth2 = []\n",
    "\n",
    "cfdet.generator.eval()\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence, sequence_label, key_label in val_loader: \n",
    "        key_label_list = key_label.tolist()\n",
    "\n",
    "        for j in range(len(sequence_label)):\n",
    "            y_key_truth2 = y_key_truth2 + key_label_list[j]\n",
    "\n",
    "        sequence = sequence.cuda()\n",
    "        z_out, _ = cfdet.generate(sequence, training=False)\n",
    "        z_list = z_out.data.tolist()\n",
    "\n",
    "        for k in range(len(sequence_label)):\n",
    "            y_key_pred2 = y_key_pred2 + z_list[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "z-PQ2h_ikWBM",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9952    0.9644    0.9796    183864\n",
      "           1     0.9131    0.9877    0.9489     69536\n",
      "\n",
      "    accuracy                         0.9708    253400\n",
      "   macro avg     0.9541    0.9760    0.9642    253400\n",
      "weighted avg     0.9727    0.9708    0.9712    253400\n",
      "\n",
      "[[177325   6539]\n",
      " [   858  68678]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_key_truth2, y_key_pred2, digits=4))\n",
    "print(metrics.confusion_matrix(y_key_truth2, y_key_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfdet.generator.load_state_dict(torch.load('state_dict_minloss.bin')) \n",
    "\n",
    "y_key_pred = []\n",
    "y_key_truth = []\n",
    "\n",
    "cfdet.generator.eval()\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequence, sequence_label, key_label in train_test_loader2:            \n",
    "        key_label_list = torch.reshape(key_label, (-1,)).tolist()\n",
    "        y_key_truth = y_key_truth + key_label_list\n",
    "\n",
    "        sequence = sequence.cuda()\n",
    "        z_out, _ = cfdet.generate(sequence, training=False)\n",
    "        z_list = torch.reshape(z_out, (-1,)).tolist()\n",
    "\n",
    "        y_key_pred = y_key_pred + z_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9747    0.9809    0.9778    105791\n",
      "           1     0.9968    0.9957    0.9962    622909\n",
      "\n",
      "    accuracy                         0.9935    728700\n",
      "   macro avg     0.9857    0.9883    0.9870    728700\n",
      "weighted avg     0.9935    0.9935    0.9935    728700\n",
      "\n",
      "[[103773   2018]\n",
      " [  2698 620211]]\n",
      "0.9882966812797547\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_key_truth, y_key_pred, digits=4))\n",
    "print(metrics.confusion_matrix(y_key_truth, y_key_pred))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_key_truth, y_key_pred, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))\n",
    "\n",
    "f = open('output.txt', 'a')\n",
    "f.write('Entry anomaly detection on detected sequences:'+'\\n')\n",
    "f.write(str(metrics.classification_report(y_key_truth, y_key_pred, digits=4))+'\\n')\n",
    "f.write(str(metrics.confusion_matrix(y_key_truth, y_key_pred))+'\\n')\n",
    "f.write(str(metrics.auc(fpr, tpr))+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:01<00:00, 70.10it/s] \n"
     ]
    }
   ],
   "source": [
    "y_key_pred2 = []\n",
    "y_key_truth2 = []\n",
    "\n",
    "for sequence, sequence_label, key_label in tqdm(train_test_loader3):   \n",
    "    key_label_list = torch.reshape(key_label, (-1,)).tolist()\n",
    "    y_key_truth2 = y_key_truth2 + key_label_list   \n",
    "    y_key_pred2 = y_key_pred2 + [0]*len(key_label_list)\n",
    "        \n",
    "y_key_truth_all = y_key_truth + y_key_truth2\n",
    "y_key_pred_all = y_key_pred + y_key_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9957    0.9988    0.9972   1652987\n",
      "           1     0.9968    0.9886    0.9927    627373\n",
      "\n",
      "    accuracy                         0.9960   2280360\n",
      "   macro avg     0.9962    0.9937    0.9949   2280360\n",
      "weighted avg     0.9960    0.9960    0.9960   2280360\n",
      "\n",
      "[[1650969    2018]\n",
      " [   7162  620211]]\n",
      "0.9936816617373385\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_key_truth_all, y_key_pred_all, digits=4))\n",
    "print(metrics.confusion_matrix(y_key_truth_all, y_key_pred_all))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_key_truth_all, y_key_pred_all, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))\n",
    "\n",
    "f = open('output.txt', 'a')\n",
    "f.write('Entry anomaly detection on unlabeled dataset:'+'\\n')\n",
    "f.write(str(metrics.classification_report(y_key_truth_all, y_key_pred_all, digits=4))+'\\n')\n",
    "f.write(str(metrics.confusion_matrix(y_key_truth_all, y_key_pred_all))+'\\n')\n",
    "f.write(str(metrics.auc(fpr, tpr))+'\\n')\n",
    "f.write('-'*50+'\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Rationale_Thunderbird.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
